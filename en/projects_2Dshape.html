<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Intelligent and Multimedia Science Laboratory</title>
		<link rel="icon" href="../files/images/logo.png">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<meta name="sysuimsl" content="Intelligent and Multimedia Science Laboratory" />
		<!--[if lte IE 8]><script src="../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../assets/css/main-v20210707.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="../assets/css/ie8.css" /><![endif]-->
		<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
		<script>
		  (adsbygoogle = window.adsbygoogle || []).push({
		    google_ad_client: "ca-pub-6919411062262434",
		    enable_page_level_ads: true
		  });
		</script>
	</head>
	<script src="//instant.page/1.1.0" type="module" integrity="sha384-EwBObn5QAxP8f09iemwAJljc+sU+eUXeL9vSBw1eNmVarwhKk2F9vBEpaN9rsrtp"></script>
	<body class="is-preload">
	    <div id="wrapper">
		<!-- Main -->
			<div id="main">
		    	<div class="inner">
					<!-- Header -->
					<header id="header">
						<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                            <tr class="spec">
                                <td width="70%">
									<h11 class="head">Intelligent and Multimedia Science Laboratory</h11>
                                </td>
                                <td width="30%">
								    <ul class="icons">
										<li><a href="https://github.com/sysu-imsl" class="icon fa-github"><span class="label">Github</span></a></li>
										<li><a href="mailto:sysuimsl@126.com" class="icon fa-envelope-o"><span class="label">Email</span></a></li>
										<li><a href="../projects_2Dshape.html">中文版 ☞</a></li>
									</ul>
                                </td>
                            </tr>
                        </table>
					</header>

					<br>
                    <ul class="iconstext">
                        <li><a class="mybuttontop" href="projects_indoor.html">Intelligent Indoor Spatial Sensing</a></li>
                        <li><a class="mybuttontop" href="projects_attack.html">Artificial Intelligence Security</a></li>
                        <li><a class="mybuttonnow">2D/3D Shape and Image Processing</a></li>
                    </ul>
                    <br>

                    <ul class="iconstext">
                        <li><a class="mybuttonlow" href="#sketch">Sketch Generation and Applications</a></li>
                        <li><a class="mybuttonlow" href="#editing_and_synthesis">Image Editing and Synthesis</a></li>
                        <li><a class="mybuttonlow" href="#pose">3D Pose Estimation and Motion Generation</a></li>
                        <li><a class="mybuttonlow" href="#fashion">Garment Modeling and Virtual Try-on</a></li>
                        <li><a class="mybuttonlow" href="#3d_modeling">3D Rendering & Modeling</a></li>
                    </ul>

					<section id="sketch">
						<header class="major">
							<h2>Sketch Generation and Applications</h2>
						</header>

                        <table style="margin-bottom: 0px;" width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                        <tr class="spec">
                            <td width="39%" valign="top" align="center">
                                <img class="image fit thumb" src="../files/images/projects/muten-fast.gif" alt="" style="margin-bottom: 0px;" width="100%" border="0">
                            </td>

                            <td width="17%" valign="top" align="center">
                                <img class="image fit thumb"  src="../files/images/projects/rocket.png" alt="" style="margin-bottom: 0px;" width="100%" border="0">
                                <p class="margin-small">&nbsp;</p>
                                <img src="../files/images/projects/1390_input.png" width="80%">
                            </td>

                            <td width="17%" valign="top" align="center">
                                <img class="image fit thumb"  src="../files/images/projects/rocket-blue-simplest.gif" alt="" style="margin-bottom: 0px;" width="100%" border="0">
                                <p class="margin-small">&nbsp;</p>
                                <img src="../files/images/projects/face-blue-1390-simplest.gif" width="80%">
                            </td>

                            <td width="27%" valign="top" align="center">
                                <img class="image fit thumb"  src="../files/images/projects/color.gif" alt="" style="margin-bottom: 0px;" width="100%" border="0">
                            </td>
                        </tr>
                        </table>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/pg2022.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://diglib.eg.org/handle/10.2312/pg20221238">Multi-instance Referring Image Segmentation of Scene Sketches based on Global Reference Mechanism</a></h3>
                                    <p>
                                        Peng Ling, Haoran Mo and Chengying Gao<sup>*</sup>
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        We propose GRM-Net, a one-stage framework tailored for multi-instance referring image segmentation of scene sketches.
                                        We extract the language features from the expression and fuse it into a conventional instance segmentation pipeline
                                        for filtering out the undesired instances in a coarse-to-fine manner and keeping the matched ones.
                                        To model the relative arrangement of the objects and the relationship among them from a global view,
                                        we propose a global reference mechanism (GRM) to assign references to each detected candidate to identify its position.
                                        <br>
                                        <br>
                                        <em>Pacific Graphics (<strong>PG</strong> 2022) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="../files/PG2022/referring_sketch_segmentation.pdf">[Paper]</a>
                                        <a href="https://github.com/LQY404/GRM-Net">[Code]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/colorization-PG2021.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="../files/PG2021/line_art_colorization_pg2021_main.pdf">Line Art Colorization Based on Explicit Region Segmentation</a></h3>
                                    <p>
                                        Ruizhi Cao, Haoran Mo and Chengying Gao<sup>*</sup>
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        We introduce an explicit segmentation fusion mechanism to aid colorization frameworks in avoiding color bleeding artifacts.
                                        This mechanism is able to provide region segmentation information for the colorization process explicitly
                                        so that the colorization model can learn to avoid assigning the same color across regions with different semantics
                                        or inconsistent colors inside an individual region.
                                        The proposed mechanism is designed in a plug-and-play manner,
                                        so it can be applied to a diversity of line art colorization frameworks with various kinds of user guidances.
                                        <br>
                                        <br>
                                        <em>Computer Graphics Forum (<strong>Pacific Graphics</strong> 2021) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="../files/PG2021/line_art_colorization_pg2021_main.pdf">[Paper]</a>
                                        <a href="https://github.com/Ricardo-L-C/ColorizationWithRegion">[Code]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/siggraph2021.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://markmohr.github.io/virtual_sketching/">General Virtual Sketching Framework for Vector Line Art</a></h3>
                                    <p>
                                        Haoran Mo, Edgar Simo-Serra, Chengying Gao<sup>*</sup>, Changqing Zou and Ruomei Wang
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        Vector line art plays an important role in graphic design, however,
                                        it is tedious to manually create.
                                        We introduce a general framework to produce line drawings from a wide variety of images,
                                        by learning a mapping from raster image space to vector image space.
                                        Our approach is based on a recurrent neural network that draws the lines one by one.
                                        A differentiable rasterization module allows for training with only supervised raster data.
                                        We use a dynamic window around a virtual pen while drawing lines,
                                        implemented with a proposed aligned cropping and differentiable pasting modules.
                                        Furthermore, we develop a stroke regularization loss that encourages the model
                                        to use fewer and longer strokes to simplify the resulting vector image.
                                        Ablation studies and comparisons with existing methods corroborate the efficiency of our approach
                                        which is able to generate visually better results in less computation time,
                                        while generalizing better to a diversity of images and applications.
                                        <br>
                                        <br>
                                        <em>ACM Transactions on Graphics (<strong>SIGGRAPH</strong> 2021, Journal track) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="https://esslab.jp/publications/HaoranSIGRAPH2021.pdf">[Paper]</a>
                                        <a href="https://github.com/MarkMoHR/virtual_sketching">[Code]</a>
                                        <a href="https://markmohr.github.io/virtual_sketching/">[Project Page]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/sketchycoco.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://arxiv.org/abs/2003.02683">SketchyCOCO: Image Generation from Freehand Scene Sketches</a></h3>
                                    <p>
                                        Chengying Gao, Qi Liu, Qi Xu, Jianzhuang Liu, Limin Wang, Changqing Zou*
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        We introduce the first method for automatic image generation from scene-level freehand sketches.
                                        Our model allows for controllable image generation by specifying the synthesis goal
                                        via freehand sketches. The key contribution is an attribute vector bridged
                                        generative adversarial network called edgeGAN which supports high visual-quality image content
                                        generation without using freehand sketches as training data.
                                        We build a large-scale composite dataset called SketchyCOCO to comprehensively
                                        evaluate our solution. We validate our approach on the task of both objectlevel
                                        and scene-level image generation on SketchyCOCO. We demonstrate the method’s capacity
                                        to generate realistic complex scene-level images from a variety of freehand sketches
                                        by quantitative, qualitative results, and ablation studies.
                                        <br>
                                        <br>
                                        <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>, 2020) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="https://arxiv.org/abs/2003.02683">[Paper]</a>
                                        <a href="https://github.com/sysu-imsl/SketchyCOCO">[Code]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/colorization2.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://github.com/SketchyScene/SketchySceneColorization">Language-based Colorization of Scene Sketches</a></h3>
                                    <p>
                                        Changqing Zou<sup>#</sup>, Haoran Mo<sup>#</sup>(joint first author), Chengying Gao<sup>*</sup>, Ruofei Du and Hongbo Fu
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        This paper for the first time presents a language-based system
                                        for interactive colorization of scene sketches, based on semantic comprehension.
                                        The proposed system is built upon deep neural networks
                                        trained on a large-scale repository of scene sketches
                                        and cartoonstyle color images with text descriptions.
                                        Given a scene sketch, our system allows users, via language-based instructions,
                                        to interactively localize and colorize specific foreground object instances
                                        to meet various colorization requirements in a progressive way.
                                        We demonstrate the effectiveness of our approach via comprehensive experimental results
                                        including alternative studies, comparison with the state-of-the-art methods,
                                        and generalization user studies. Given the unique characteristics of language-based inputs,
                                        we envision a combination of our interface with a traditional scribble-based interface
                                        for a practical multimodal colorization system, benefiting various applications.
                                        <br>
                                        <br>
                                        <em>ACM Transactions on Graphics (<strong>SIGGRAPH Asia</strong> 2019, Journal track) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="http://mo-haoran.com/files/SIGA19/SketchColorization_paper_SA2019.pdf">[Paper]</a>
                                        <a href="https://github.com/SketchyScene/SketchySceneColorization">[Code]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/sketchyscene.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://sketchyscene.github.io/SketchyScene/">SketchyScene: Richly-Annotated Scene Sketches</a></h3>
                                    <p>
                                        Changqing Zou<sup>#</sup>, Qian Yu<sup>#</sup>, Ruofei Du, Haoran Mo, Yi-Zhe Song, Tao Xiang, Chengying Gao, Baoquan Chen<sup>*</sup>, and Hao Zhang
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        This paper constructed the first large-scale dataset of scene sketches called SketchyScene.
                                        We demonstrate the potential impact of SketchyScene by training
                                        new computational models for semantic segmentation of scene sketches.
                                        <br>
                                        <br>
                                        <em>European Conference on Computer Vision (<strong>ECCV</strong>, 2018) &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper.pdf">[Paper]</a>
                                        <a href="https://github.com/SketchyScene/SketchyScene">[Code]</a>
                                    </p>
                                </td>
                            </tr>

                        </table>


					</section>

                    <div align="right">
                        <a href="#">Back to top</a>
                    </div>

                    <section id="editing_and_synthesis">
						<header class="major">
							<h2>Image Editing and Synthesis</h2>
						</header>

                        <p>
                            Including: image inpainting, color restoration, color transfer and non-photorealistic rendering.
                        </p>
                        <br>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/cvpr2023.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://arxiv.org/abs/2303.17867">CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer</a></h3>
                                    <p>
                                        Linfeng Wen, Chengying Gao*, Changqing Zou
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        Content affinity loss including feature and pixel affinity is a main problem which leads to artifacts in photorealistic and video style transfer.
                                        This paper proposes a new framework named CAP-VSTNet, which consists of a new reversible residual network and an unbiased linear transform module,
                                        for versatile style transfer. This reversible residual network can not only preserve content affinity but not introduce redundant information as traditional reversible networks,
                                        and hence facilitate better stylization.
                                        Empowered by Matting Laplacian training loss which can address the pixel affinity loss problem led by the linear transform,
                                        the proposed framework is applicable and effective on versatile style transfer.
                                        <br>
                                        <br>
                                        <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>, 2023) <strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="https://arxiv.org/abs/2303.17867">[Paper]</a>
                                        <a href="https://github.com/linfengWen98/CAP-VSTNet">[Code]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/icme2021inpainting.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://ieeexplore.ieee.org/abstract/document/9428367">Structural Prior Guided Image Inpainting for Complex Scene</a></h3>
                                    <p>
                                        Shuxin Wei, Chengying Gao
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        Existing deep-learning based image inpainting methods have reach plausible results
                                        for small corrupted regions with rich context information.
                                        However, these methods fail to generate semantically reasonable results and clear boundaries.
                                        In this paper, we disentangle inpainting for complex scene into two stages:
                                        semantic segmentation map inpainting and segmentation guided texture inpainting.
                                        We use feature correspondence matrix to find correlation between segmentation maps
                                        and known region of corrupted images and realize texture generation of corrupted region.
                                        <br>
                                        <br>
                                        <em>International Conference on Multimedia & Expo (<strong>ICME</strong>, 2021)  <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://ieeexplore.ieee.org/abstract/document/9428367">[Paper]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/jisuanjixuebao19.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="">基于稀疏结构的复杂物体修复</a></h3>
                                    <p>
                                        高成英，徐仙儿，罗燕媚，王栋
                                        <br>
                                        <br>
                                        <em>计算机学报</em>，2019
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/Neurocomputing18.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://www.sciencedirect.com/science/article/pii/S0925231218306672">An edge-refined vectorized deep colorization model for grayscale-to-color images</a></h3>
                                    <p>
                                        Zhuo Su, Xiangguo Liang, Jiaming Guo, Chengying Gao, Xiaonan Luo
                                        <br>
                                        <br>
                                        <em>Neurocomputing</em>, 2018
                                        <br>
                                        <a href="https://www.sciencedirect.com/science/article/pii/S0925231218306672">[Paper]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/pencil-art.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%" class="tdp">
								    <h3> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13334">PencilArt: A Chromatic Penciling Style Generation Framework</a></h3>
                                    <p>
                                        Chengying Gao, Mengyue Tang, Xiangguo Liang, Zhou Su, Changqing Zou
                                        <br>
                                        <br>
                                        <em>Computer Graphics Forum (<strong>CGF</strong>), 2018 &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13334">[Paper]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/spic17.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://www.sciencedirect.com/science/article/pii/S0923596517300772">Data-Driven Image Completion for Complex Object</a></h3>
                                    <p>
                                        Chengying Gao, Yanmei Luo, Hefeng Wu*, Dong Wang
                                        <br>
                                        <br>
                                        <em>Signal Processing: Image Communication</em>, 2017
                                        <br>
                                        <a href="https://www.sciencedirect.com/science/article/pii/S0923596517300772">[Paper]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/color-transfer.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://changqingzou.weebly.com/uploads/5/9/3/6/59369643/wang_et_al-2017-computer_graphics_forum-min.pdf">L0 Gradient-Preserving Color Transfer</a></h3>
                                    <p>
                                        Dong Wang, Changqing Zou, Guiqing Li, Chengying Gao, Zhuo Su, Ping Tan
                                        <br>
                                        <br>
                                        <em>Computer Graphics Forum (<strong>CGF</strong>), 2017 &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://changqingzou.weebly.com/uploads/5/9/3/6/59369643/wang_et_al-2017-computer_graphics_forum-min.pdf">[Paper]</a>
                                    </p>
                                </td>
                            </tr>

                        </table>

					</section>

                    <div align="right">
                        <a href="#">Back to top</a>
                    </div>

                    <section id="pose">
						<header class="major">
							<h2>3D Pose Estimation and Motion Generation</h2>
						</header>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/icme2022-4.gif" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="../files/ICME2022/Unpaired_Motion_Style_Transfer_camera_ready.pdf">Unpaired Motion Style Transfer with Motion-oriented Projection Flow Network</a></h3>
                                    <p>
                                        Yue Huang, Haoran Mo, Xiao Liang, Chengying Gao*
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        In this paper, we propose a novel unpaired motion style transfer framework that
                                        generates complete stylized motions with consistent content.
                                        We introduce a motion-oriented projection flow network (M-PFN) designed for temporal motion data,
                                        which encodes the content and style motions into latent codes
                                        and decodes the stylized features produced by adaptive instance normalization (AdaIN) into stylized motions.
                                        The M-PFN contains dedicated operations and modules, e.g., Transformer, to process the temporal information of motions,
                                        which help to improve the continuity of the generated motions.
                                        <br>
                                        <br>
                                        <em>International Conference on Multimedia & Expo (<strong>ICME</strong>, 2022) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="../files/ICME2022/Unpaired_Motion_Style_Transfer_camera_ready.pdf">[Paper]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/neurocomputing2022.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://doi.org/10.1016/j.neucom.2021.12.013">3D interacting hand pose and shape estimation from a single RGB image</a></h3>
                                    <p>
                                        Chengying Gao*, Yujia Yang, Wensheng Li
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        This paper proposes a network called GroupPoseNet using a grouping strategy to address this problem.
                                        GroupPoseNet extracts the left- and right-hand features respectively and thus avoids the mutual affection between the interacting hands.
                                        Empowered by a novel up-sampling block called MF-Block predicting 2D heat-maps in a progressive way by fusing image features,
                                        hand pose features, and multi-scale features, GroupPoseNet is effective and robust to severe occlusions.
                                        To achieve an effective 3D hand reconstruction, we design a transformer mechanism based inverse kinematics module(termed TikNet) to generate 3D hand mesh.
                                        <br>
                                        <br>
                                        <em>Neurocomputing, 2022</em>
                                        <br>
                                        <a href="https://doi.org/10.1016/j.neucom.2021.12.013">[Paper]</a>
                                    </p>
                                </td>
                            </tr>


                        </table>
					</section>

                    <div align="right">
                        <a href="#">Back to top</a>
                    </div>

                    <section id="fashion">
                        <header class="major">
							<h2>Garment Modeling and Virtual Try-on</h2>
						</header>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/pg2023.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="">Controllable Garment Image Synthesis Integrated with Frequency Domain Features</a></h3>
                                    <p>	Xinru Liang, Haoran Mo, Chengying Gao*
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        We propose a controllable garment image synthesis framework
                                        that takes as inputs an outline sketch and a texture patch and generates garment images with complicated and diverse texture patterns.
                                        To improve the performance of global texture expansion, we exploit the frequency domain features in the generative process,
                                        which are from a Fast Fourier Transform (FFT) and able to represent the periodic information of the patterns.
                                        We also introduce a perceptual loss in the frequency domain to measure the similarity of two texture pattern patches
                                        in terms of their intrinsic periodicity and regularity.
                                        <br>
                                        <br>
                                        <em>Computer Graphics Forum (<strong>Pacific Graphics</strong>, 2023) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/fashionGAN.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13552">FashionGAN: Display your fashion design using Conditional Generative Adversarial Nets</a></h3>
                                    <p>	Yirui Cui, Qi Liu, Chengying Gao*, Zhuo Su
                                        <br>
                                        <br>
                                        <em>Computer Graphics Forum (<strong>Pacific Graphics</strong>, 2018) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13552">[Paper]</a>
                                        <a href="https://github.com/Cuiyirui/FashionGAN">[Code]</a>
										<a href="https://drive.google.com/drive/folders/1DACqCXlJRQxRysO6RVNO7vOoR8YzrjTQ?usp=sharing">[Dataset]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/pcm18.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://link.springer.com/chapter/10.1007%2F978-3-030-00776-8_25">Automatic 3D Garment Fitting Based on Skeleton Driving</a></h3>
                                    <p> Haozhong Cai, Guangyuan Shi, Chengying Gao*, Dong Wang
                                        <br>
                                        <br>
                                        <em>Pacific-Rim Conference on Multimedia (<strong>PCM</strong>, 2018) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-C)</font></strong></em>
                                        <br>
                                        <a href="https://link.springer.com/chapter/10.1007%2F978-3-030-00776-8_25">[Paper]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/pg14.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://diglib.eg.org/handle/10.2312/pgs.20141250.043-048">Automatic Garment Modeling From Front And Back Images</a></h3>
                                    <p>	Lifeng Huang, Chengying Gao*
                                        <br>
                                        <br>
                                        <em>Pacific Graphics (<strong>PG</strong>, 2014) &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://diglib.eg.org/handle/10.2312/pgs.20141250.043-048">[Paper]</a>
                                    </p>
                                </td>
                            </tr>

                        </table>

                    </section>

                    <div align="right">
                        <a href="#">Back to top</a>
                    </div>

					<section id="3d_modeling">
                        <header class="major">
							<h2>3D Rendering & Modeling</h2>
						</header>

                        <p>
                            Including: fast fluid surface reconstruction based on narrow band method, fabric modeling and rendering.
                        </p>
                        <br>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/cgi2020.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%" class="tdp">
								    <h3> <a href="https://link.springer.com/article/10.1007/s00371-020-01898-2">A Completely Parallel Surface Reconstruction Method for Particle-Based Fluids</a></h3>
                                    <p>
										Wencong Yang, Chengying Gao
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        In this paper, a fast, simple and extremely accurate narrow-band method of
										fluid surface is proposed firstly, which makes the surface reconstruction algorithm
										(such as marching cube) accurately process the valid fluid surface area,
										which greatly avoids the useless calculation process. At the same time,
										we analyze the potential race conditions and conditional branching in the reconstruction process,
										by using mutual exclusive prefix sum algorithm,
										the whole process of fluid surface reconstruction is completely parallelized,
										which greatly speeds up the efficiency of surface reconstruction.
                                        <br>
                                        <br>
                                        <em>Computer Graphics International (<strong>CGI</strong>, 2020) &nbsp;<strong><font color="#FF0000">(CCF-C)</font></strong></em>
                                        <br>
                                        <a href="https://link.springer.com/article/10.1007/s00371-020-01898-2">[Paper]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/fabric-model.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%" class="tdp">
								    <h3> <a href="../files/3D-materials/fabric-model.pdf">Fully automatic algorithm on yarn model generation</a></h3>
                                    <p>
										Zekun Zhang
                                        <br>
                                        <br>
                                        <a href="../files/3D-materials/fabric-model.pdf">[Introduction (PPT)]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/fabric-render.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%" class="tdp">
								    <h3> <a href="../files/3D-materials/fabric-render.pdf">Microscopic model based real time algorithm on fabric rendering</a></h3>
                                    <p>
										Xingrong Luo
                                        <br>
                                        <br>
                                        <a href="../files/3D-materials/fabric-render.pdf">[Introduction (PPT)]</a>
                                    </p>
                                </td>
                            </tr>


                        </table>


                    </section>
                    
                    <div align="right">
                        <a href="#">Back to top</a>
                        <br>
                        <p> &nbsp; </p>
                    </div>

				</div>
			</div>
			
			<!-- Sidebar -->
				<div id="sidebar">
					<div class="inner">

						<!-- Search -->
							<section id="search" class="alt">
								<!--<form method="post" action="#">-->
									<!--<input type="text" name="query" id="query" placeholder="Search" />-->
								<!--</form>-->
                                <div style="text-align:center" >
                                    <img src="../files/images/logo2.png" alt="" width="100%"/>
                                </div>
							</section>

						<!-- Menu -->
							<nav id="menu">
								<header class="major">
									<h2>Menu</h2>
								</header>
								<ul>
									<li><a href="index.html">Home</a></li>
									<li><a href="projects_2Dshape.html">Projects</a></li>
									<li><a href="publications.html">Publications</a></li>
									<li><a href="members.html">Members</a></li>
									<li><a href="students.html">Student Development</a></li>
									<li><a href="materials_datasets.html">Related Materials</a></li>
									<!--
									<li>
										<span class="opener">Submenu</span>
										<ul>
											<li><a href="#">Lorem Dolor</a></li>
											<li><a href="#">Ipsum Adipiscing</a></li>
											<li><a href="#">Tempus Magna</a></li>
											<li><a href="#">Feugiat Veroeros</a></li>
										</ul>
									</li>
									<li><a href="#">Etiam Dolore</a></li>
									<li><a href="#">Adipiscing</a></li>
									<li>
										<span class="opener">Another Submenu</span>
										<ul>
											<li><a href="#">Lorem Dolor</a></li>
											<li><a href="#">Ipsum Adipiscing</a></li>
											<li><a href="#">Tempus Magna</a></li>
											<li><a href="#">Feugiat Veroeros</a></li>
										</ul>
									</li>
									-->
								</ul>
							</nav>

						<!-- Section -->
						<!--
							<section>
								<header class="major">
									<h2>Ante interdum</h2>
								</header>
								<div class="mini-posts">
									<article>
										<a href="#" class="image"><img src="./images/pic07.jpg" alt="" /></a>
										<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
									</article>
									<article>
										<a href="#" class="image"><img src="./images/pic08.jpg" alt="" /></a>
										<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
									</article>
									<article>
										<a href="#" class="image"><img src="./images/pic09.jpg" alt="" /></a>
										<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
									</article>
								</div>
								<ul class="actions">
									<li><a href="#" class="button">More</a></li>
								</ul>
							</section>
                        -->
						<!-- Section -->
							<section>
								<header class="major">
									<h2>Get in touch</h2>
								</header>
								<ul class="contact">
									<li class="fa-envelope-o">Email: <a href="#">sysuimsl@126.com</a></li>
									<li class="fa-home">Address: <a href="https://goo.gl/maps/P7iu1XtZfrk5TsKz5">Room A409, School Of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China</a>
									</li>
								</ul>
								<!--<article class="5u 10u$(xsmall) work-item">-->
										<!--<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&d=mhnrYabZI2bz_eHk1W_A8VvNxtAjYBrWfIfxbLnTRPQ&co=4c5459&cmo=faa659'></script>-->
								<!--</article>-->
								<!--<a href="https://info.flagcounter.com/Dsap"><img src="https://s11.flagcounter.com/count2/Dsap/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>-->
							</section>

						<!-- Footer -->
							<footer id="footer">
								<p class="copyright">
									<li>Copyright &copy; 2019-2023 SYSU-IMSL</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</footer>

					</div>
				</div>
        <!-- push the page to Baidu -->
		<script>
		(function(){
		    var bp = document.createElement('script');
		    var curProtocol = window.location.protocol.split(':')[0];
		    if (curProtocol === 'https'){
		   bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
		  }
		  else{
		  bp.src = 'http://push.zhanzhang.baidu.com/push.js';
		  }
		    var s = document.getElementsByTagName("script")[0];
		    s.parentNode.insertBefore(bp, s);
		})();
		</script>
		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>
	</body>
</html>

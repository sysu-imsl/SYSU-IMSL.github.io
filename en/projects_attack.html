<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Intelligent and Multimedia Science Laboratory</title>
		<link rel="icon" href="../files/images/logo.png">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<meta name="sysuimsl" content="Intelligent and Multimedia Science Laboratory" />
		<!--[if lte IE 8]><script src="../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../assets/css/main-v20210707.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="../assets/css/ie8.css" /><![endif]-->
		<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
		<script>
		  (adsbygoogle = window.adsbygoogle || []).push({
		    google_ad_client: "ca-pub-6919411062262434",
		    enable_page_level_ads: true
		  });
		</script>
	</head>
	<script src="//instant.page/1.1.0" type="module" integrity="sha384-EwBObn5QAxP8f09iemwAJljc+sU+eUXeL9vSBw1eNmVarwhKk2F9vBEpaN9rsrtp"></script>
	<body class="is-preload">
	    <div id="wrapper">
		<!-- Main -->
			<div id="main">
		    	<div class="inner">
					<!-- Header -->
					<header id="header">
						<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                            <tr class="spec">
                                <td width="70%">
									<h11 class="head">Intelligent and Multimedia Science Laboratory</h11>
                                </td>
                                <td width="30%">
								    <ul class="icons">
										<li><a href="https://github.com/sysu-imsl" class="icon fa-github"><span class="label">Github</span></a></li>
										<li><a href="mailto:sysuimsl@126.com" class="icon fa-envelope-o"><span class="label">Email</span></a></li>
										<li><a href="../projects_attack.html">中文版 ☞</a></li>
									</ul>
                                </td>
                            </tr>
                        </table>
					</header>

					<br>

                    <ul class="iconstext">
                        <li><a class="mybuttontop" href="projects_indoor.html">Intelligent Indoor Spatial Sensing</a></li>
                        <li><a class="mybuttonnow">Artificial Intelligence Security</a></li>
                        <li><a class="mybuttontop" href="projects_2Dshape.html">2D/3D Shape and Image Processing</a></li>
                    </ul>
					<br>

                    <ul class="iconstext">
                        <li><a class="mybuttonlow" href="#attack">Adversarial Attacks and Defenses</a></li>
                        <li><a class="mybuttonlow" href="#video">Crowd Counting</a></li>
                    </ul>


                    <section id="attack">
                        <header class="major">
							<h2>Adversarial Attacks and Defenses</h2>
						</header>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
							<tr>
                                <td width="30%">
								    <img src="../files/images/projects/attack-pr2022.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://www.sciencedirect.com/science/article/pii/S0031320322003120?dgcid=coauthor">Cyclical Adversarial Attack Pierces Black-box Deep Neural Networks</a></h3>
                                    <p>
										Lifeng Huang, Shuxin Wei, Chengying Gao, Ning Liu
										<br>
                                        <br>
                                        <strong>Intro: </strong>
                                        In this paper, we propose Cyclical Adversarial Attack (CA2),
                                        a general and straightforward method to boost the transferability to break defenders.
                                        We first revisit the momentum-based methods from the perspective of optimization and find that they usually suffer from the transferability saturation dilemma.
                                        To address this, CA2 performs cyclical optimization algorithm to produce adversarial examples.
                                        Unlike the standard momentum policy that accumulates the velocity to continuously update the solution,
                                        we divide the generation process into multiple phases and treat the velocity vectors from the previous phase as proper knowledge to guide a new adversarial attack with larger steps.
                                        Moreover, CA2 applies a novel and compatible augmentation algorithm at every optimization in a loop manner for enhancing the black-box transferability further, referred to as cyclical augmentation.
                                        <br>
                                        <br>
                                        <em>Pattern Recognition (<strong>PR</strong>)</em>, 2022
                                        <br>
                                        <a href="https://www.sciencedirect.com/science/article/pii/S0031320322003120?dgcid=coauthor">[Paper]</a>
                                        <a href="https://github.com/mesunhlf/CA2">[Code]</a>
                                    </p>
                                </td>
                            </tr>

							<tr>
                                <td width="30%">
								    <img src="../files/images/projects/icme2021attack.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://github.com/zhuangwz/ICME2021_self_augmentation">Enhancing Adversarial Examples Via Self-Augmentation</a></h3>
                                    <p>
										Lifeng Huang, Wenzi Zhuang, Chengying Gao, Ning Liu
										<br>
                                        <br>
                                        <strong>Intro: </strong>
                                        Recently, adversarial attacks pose a challenge for the security of Deep Neural Networks,
										which motivates researchers to establish various defense methods.
										However, do current defenses achieve real security enough?
										To answer the question, we propose self-augmentation method (SA)
										for circumventing defenders to transferable adversarial examples.
										Concretely, self-augmentation includes two strategies:
										(1) self-ensemble, which applies additional convolution layers to an existing model
										to build diverse virtual models that be fused for achieving an ensemble-model effect
										and preventing overfitting; and
										(2) deviation-augmentation, which based on the observation of defense models
										that the input data is surrounded by highly curved loss surfaces,
										thus inspiring us to apply deviation vectors to input data for escaping from their vicinity space.
										Notably, we can naturally combine self-augmentation with existing methods
										to establish more transferable adversarial attacks.
										Extensive experiments conducted on four vanilla models and ten defenses suggest the superiority of our method
										compared with the state-of-the-art transferable attacks.
                                        <br>
                                        <br>
                                        <em>International Conference on Multimedia & Expo (<strong>ICME</strong>, 2021)  <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://ieeexplore.ieee.org/abstract/document/9428372">[Paper]</a>
                                        <a href="https://github.com/zhuangwz/ICME2021_self_augmentation">[Code]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/attack-cvpr2020.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://arxiv.org/pdf/1909.04326">Universal Physical Camouflage Attacks on Object Detectors</a></h3>
                                    <p>
										Lifeng Huang, Chengying Gao, Yuyin Zhou, Changqing Zou, Cihang Xie, Alan Yuille, Ning Liu
										<br>
                                        <br>
                                        <strong>Intro: </strong>
                                        In this paper, we study physical adversarial attacks on object detectors in the wild.
										Previous works on this matter mostly craft instance-dependent perturbations
										only for rigid and planar objects.
										To this end, we propose to learn an adversarial pattern to effectively
										attack all instances belonging to the same object category (e.g., person, car),
										referred to as Universal Physical Camouflage Attack (UPC).
										Concretely, UPC crafts camouflage by jointly fooling the region proposal network,
										as well as misleading the classifier and the regressor to output errors.
										In order to make UPC effective for articulated non-rigid or non-planar objects,
										we introduce a set of transformations for the generated camouflage patterns to
										mimic their deformable properties.
										We additionally impose optimization constraint to make generated patterns look
										natural to human observers. To fairly evaluate the effectiveness of different
										physical-world attacks on object detectors, we present the first standardized
										virtual database, AttackScenes, which simulates the real 3D world in a controllable
										and reproducible environment. Extensive experiments suggest the superiority of
										our proposed UPC compared with existing physical adversarial attackers not only
										in virtual environments (AttackScenes), but also in real-world physical environments.
                                        <br>
                                        <br>
                                        <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>, 2020) &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Universal_Physical_Camouflage_Attacks_on_Object_Detectors_CVPR_2020_paper.pdf">[Paper]</a>
                                        <a href="https://mesunhlf.github.io/index_physical.html">[Project Page]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/attack-guap.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="http://www.acml-conf.org/2019/conference/accepted-papers/419/?from=singlemessage&isappinstalled=0&scene=1&clicktime=1577243300&enterid=1577243300">G-UAP: Generic Universal Adversarial Perturbation that Fools RPN-based Detectors</a></h3>
                                    <p>
										Xing wu, Lifeng Huang, Chengying Gao*
										<br>
                                        <br>
                                        <strong>Intro: </strong>
                                        Our paper proposed the G-UAP which is the first work to craft universal
                                        adversarial perturbations to fool the RPN-based detectors. G-UAP focuses
                                        on misleading the foreground prediction of RPN to background to make detectors
                                        detect nothing.
                                        <br>
                                        <br>
                                        <em>Asian Conference on Machine Learning (<strong>ACML</strong>, 2019) &nbsp;<strong><font color="#FF0000">(CCF-C)</font></strong></em>
										<br>
                                        <a href="http://www.acml-conf.org/2019/conference/accepted-papers/419/?from=singlemessage&isappinstalled=0&scene=1&clicktime=1577243300&enterid=1577243300">[Paper]</a>
                                    </p>
                                </td>
                            </tr>

                        </table>

                    </section>

                    <div align="right">
                        <a href="#">Back to top</a>
                    </div>

                    <section id="video">
                        <header class="major">
							<h2>Crowd Counting</h2>
						</header>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/acmmm2020.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413698">Scale-aware Progressive Optimization Network</a></h3>
                                    <p>
                                        Ying Chen, Lifeng Huang, Chengying Gao, Ning Liu
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        Crowd counting has attracted increasing attention due to its wide application prospect.
                                        One of the most essential challenge in this domain is large scale variation,
                                        which impacts the accuracy of density estimation.
                                        To this end, we propose a scale-aware progressive optimization network (SPO-Net)
                                        for crowd counting, which trains a scale adaptive network to achieve high-quality density map estimation
                                        and overcome the variable scale dilemma in highly congested scenes.
                                        Concretely, the first phase of SPO-Net, band-pass stage, mainly concentrates on preprocessesing the input image
                                        and fusing both high-level semantic information and low-level spatial information from separated multi-layer features.
                                        And the second phase of SPO-Net, rolling guidance stage,
                                        aims to learn a scale-adapted network from multi-scale features as well as rolling training manner.
                                        For better learning local correlation of multi-size regions and reducing redundant calculations,
                                        we introduce different supervisions with analogy objective in each rolling,
                                        refer to as progressive optimization strategy.
                                        Extensive experiments on three challenging crowd counting datasets (ShanghaiTech, UCF_CC_50 and UCF-QNRF)
                                        not only demonstrate the efficacy of each part in SPO-Net, but also suggest the superiority
                                        of our proposed method compared with the state-of-the-art approaches.
                                        <br>
                                        <br>
                                        <em>ACM MultiMedia (<strong>ACM MM</strong>, 2020) &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413698">[Paper]</a>

                                    </p>
                                </td>
                            </tr>

							<tr>
                                <td width="30%">
								    <img src="../files/images/projects/icme2020-fisheye.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://ieeexplore.ieee.org/abstract/document/9102923">Self-Bootstrapping Pedestrian Detection in Downward-Viewing Fisheye Cameras Using Pseudo-Labeling</a></h3>
                                    <p>
                                        Kaishi Gao, Qun Niu, Haoquan You, Chengying Gao
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        Downward-viewing fisheye cameras have attracted much attention in surveillance systems due to the wide coverage and less occlusion.
										However, pedestrian detection in downward-viewing fisheye cameras remains an open problem
										due to a lack of large-scale labeled dataset as existing datasets are
										usually based on oblique-viewing perspective cameras.
										Furthermore, it's time-consuming to label a downward-viewing fisheye dataset manually.
										To address this, we propose a self-bootstrapping pedestrian detection method,
										which automatically pseudo-labels downward-viewing fisheye images by making full use of spatial
										and temporal consistency of pedestrians in the cameras to promote the accuracy of pedestrian detection.
										We segment the downward-viewing fisheye images into two regions and propose the pseudo-labeling methods
										for them progressively: a cyclic fine-tuned detector for the oblique region and a visual tracking method
										for the vertical region. Combining the pseudo-labels from two regions,
										we fine-tune the detection network for better accuracy.
										Experimental results show that the proposed approach reduces time consumption by about 95% compared with labor-intensive manual labeling while it still reaches competitive and comparable Average Precision (AP).
                                        <br>
                                        <br>
                                        <em>International Conference on Multimedia & Expo (<strong>ICME</strong>, 2020) &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://ieeexplore.ieee.org/abstract/document/9102923/">[Paper]</a>

                                    </p>
                                </td>
                            </tr>

							<tr>
                                <td width="30%">
								    <img src="../files/images/projects/icme2020-crowd.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://ieeexplore.ieee.org/abstract/document/9102854">Scale-Aware Rolling Fusion Network for Crowd Counting</a></h3>
                                    <p>
                                        Ying Chen, Chengying Gao, Zhuo Su, Xiangjian He, Ning Liu
                                        <br>
                                        <br>
                                        <strong>Intro: </strong>
                                        Due to wide application prospects and various challenges such as large scale variation,
										inter-occlusion between crowd people and background noise,
										crowd counting is receiving increasing attention.
										In this paper, we propose a scale-aware rolling fusion network (SRF-Net) for crowd counting,
										which focuses on dealing with scale variation in highly congested noisy scenes.
										SRF-Net is a two-stage architecture that consists of a band-pass stage and a rolling guidance stage.
										Compared with the existing methods, SRF-Net achieves better results in retaining
										appropriate multi-level features and capturing multi-scale features,
										thus improving the quality of density estimation maps in crowded scenarios with large scale variation.
										We evaluate our method on three popular crowd counting datasets (ShanghaiTech, UCF_CC_50 and UCF-QNRF),
										and extensive experiments show its outperform over the state-of-the-art approaches.
                                        <br>
                                        <br>
                                        <em>International Conference on Multimedia & Expo (<strong>ICME</strong>, 2020) &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://ieeexplore.ieee.org/abstract/document/9102854/">[Paper]</a>
                                    </p>
                                </td>
                            </tr>


                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/ADCrowdNet.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_ADCrowdNet_An_Attention-Injective_Deformable_Convolutional_Network_for_Crowd_Understanding_CVPR_2019_paper.pdf">ADCrowdNet: An Attention-Injective Deformable Convolutional Network for Crowd Understanding</a></h3>
                                    <p>
                                        Ning Liu, Yongchao Long, Changqing Zou, Qun Niu, Li Pan, and Hefeng Wu
                                        <br>
                                        <br>
                                        <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>, 2019) &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_ADCrowdNet_An_Attention-Injective_Deformable_Convolutional_Network_for_Crowd_Understanding_CVPR_2019_paper.pdf">[Paper]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/video-spic18.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://www.sciencedirect.com/science/article/pii/S092359651730262X">Weak-structure-aware visual object tracking with bottom-up and top-down context exploration</a></h3>
                                    <p>
                                        Liu Ning, Liu Chang, Wu Hefeng*, and Zhu Hengzheng
                                        <br>
                                        <br>
                                        <em>Signal Processing: Image Communication (<strong>SPIC</strong>, 2018) &nbsp;<strong><font color="#FF0000">(CCF-C)</font></strong></em>
                                        <br>
                                        <a href="https://www.sciencedirect.com/science/article/pii/S092359651730262X">[Paper]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="../files/images/projects/vedio-toc15.gif" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://ieeexplore.ieee.org/document/6819051/">Hierarchical Ensemble of Background Models for PTZ-based Video Surveillance</a></h3>
                                    <p>
                                        <br>
                                        <br>
                                        <em>IEEE Transactions on Cybernetics (<strong>TCYB</strong>)</em>, 2015
                                        <br>
                                        <a href="https://ieeexplore.ieee.org/document/6819051/">[Paper]</a>
                                    </p>
                                </td>
                            </tr>

                        </table>
                    </section>

                    <div align="right">
                        <a href="#">Back to top</a>
                        <br>
                        <p> &nbsp; </p>
                    </div>

				</div>
			</div>
			
			<!-- Sidebar -->
				<div id="sidebar">
					<div class="inner">

						<!-- Search -->
							<section id="search" class="alt">
								<!--<form method="post" action="#">-->
									<!--<input type="text" name="query" id="query" placeholder="Search" />-->
								<!--</form>-->
                                <div style="text-align:center" >
                                    <img src="../files/images/logo2.png" alt="" width="100%"/>
                                </div>
							</section>

						<!-- Menu -->
							<nav id="menu">
								<header class="major">
									<h2>Menu</h2>
								</header>
								<ul>
									<li><a href="index.html">Home</a></li>
									<li><a href="projects_attack.html">Projects</a></li>
									<li><a href="publications.html">Publications</a></li>
									<li><a href="members.html">Members</a></li>
									<li><a href="students.html">Student Development</a></li>
									<li><a href="materials_datasets.html">Related Materials</a></li>
									<!--
									<li>
										<span class="opener">Submenu</span>
										<ul>
											<li><a href="#">Lorem Dolor</a></li>
											<li><a href="#">Ipsum Adipiscing</a></li>
											<li><a href="#">Tempus Magna</a></li>
											<li><a href="#">Feugiat Veroeros</a></li>
										</ul>
									</li>
									<li><a href="#">Etiam Dolore</a></li>
									<li><a href="#">Adipiscing</a></li>
									<li>
										<span class="opener">Another Submenu</span>
										<ul>
											<li><a href="#">Lorem Dolor</a></li>
											<li><a href="#">Ipsum Adipiscing</a></li>
											<li><a href="#">Tempus Magna</a></li>
											<li><a href="#">Feugiat Veroeros</a></li>
										</ul>
									</li>
									-->
								</ul>
							</nav>

						<!-- Section -->
						<!--
							<section>
								<header class="major">
									<h2>Ante interdum</h2>
								</header>
								<div class="mini-posts">
									<article>
										<a href="#" class="image"><img src="./images/pic07.jpg" alt="" /></a>
										<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
									</article>
									<article>
										<a href="#" class="image"><img src="./images/pic08.jpg" alt="" /></a>
										<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
									</article>
									<article>
										<a href="#" class="image"><img src="./images/pic09.jpg" alt="" /></a>
										<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
									</article>
								</div>
								<ul class="actions">
									<li><a href="#" class="button">More</a></li>
								</ul>
							</section>
                        -->
						<!-- Section -->
							<section>
								<header class="major">
									<h2>Get in touch</h2>
								</header>
								<ul class="contact">
									<li class="fa-envelope-o">Email: <a href="#">sysuimsl@126.com</a></li>
									<li class="fa-home">Address: <a href="https://goo.gl/maps/P7iu1XtZfrk5TsKz5">Room A409, School Of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China</a>
									</li>
								</ul>
								<!--<article class="5u 10u$(xsmall) work-item">-->
										<!--<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&d=mhnrYabZI2bz_eHk1W_A8VvNxtAjYBrWfIfxbLnTRPQ&co=4c5459&cmo=faa659'></script>-->
								<!--</article>-->
								<!--<a href="https://info.flagcounter.com/Dsap"><img src="https://s11.flagcounter.com/count2/Dsap/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>-->
							</section>

						<!-- Footer -->
							<footer id="footer">
								<p class="copyright">
									<li>Copyright &copy; 2019-2022 SYSU-IMSL</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</footer>

					</div>
				</div>
        <!-- push the page to Baidu -->
		<script>
		(function(){
		    var bp = document.createElement('script');
		    var curProtocol = window.location.protocol.split(':')[0];
		    if (curProtocol === 'https'){
		   bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
		  }
		  else{
		  bp.src = 'http://push.zhanzhang.baidu.com/push.js';
		  }
		    var s = document.getElementsByTagName("script")[0];
		    s.parentNode.insertBefore(bp, s);
		})();
		</script>
		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>
	</body>
</html>

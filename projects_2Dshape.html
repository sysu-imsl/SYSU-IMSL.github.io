<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>智能与多媒体科学实验室</title>
		<link rel="icon" href="./files/images/logo.png">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<meta name="sysuimsl" content="智能与多媒体科学实验室" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main-v20210707.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
		<script>
		  (adsbygoogle = window.adsbygoogle || []).push({
		    google_ad_client: "ca-pub-6919411062262434",
		    enable_page_level_ads: true
		  });
		</script>
	</head>
	<script src="//instant.page/1.1.0" type="module" integrity="sha384-EwBObn5QAxP8f09iemwAJljc+sU+eUXeL9vSBw1eNmVarwhKk2F9vBEpaN9rsrtp"></script>
	<body class="is-preload">
	    <div id="wrapper">
		<!-- Main -->
			<div id="main">
		    	<div class="inner">
					<!-- Header -->
					<header id="header">
						<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                            <tr class="spec">
                                <td width="70%">
									<h11 class="head-zh">智能与多媒体科学实验室</h11>
                                </td>
                                <td width="30%">
								    <ul class="icons">
										<li><a href="https://github.com/sysu-imsl" class="icon fa-github"><span class="label">Github</span></a></li>
										<li><a href="mailto:sysuimsl@126.com" class="icon fa-envelope-o"><span class="label">Email</span></a></li>
										<li><a href="en/projects_2Dshape.html">Eng. Version ☞</a></li>
									</ul>
                                </td>
                            </tr>
                        </table>
					</header>

                    <br>


                    <!--<h2>-->
                        <!--<ul class="icons">-->
							<!--<li><a href="projects_indoor_en.html">test</a></li>-->
							<!--<li><a href="projects_indoor_en.html">test1</a></li>-->
                            <!--<li><a href="projects_indoor_en.html">test3</a></li>-->
                        <!--</ul>-->
                    <!--</h2>-->

                    <ul class="iconstext">
                        <li><a class="mybuttontop" href="projects_indoor.html">室内空间位置智能感知与应用</a></li>
                        <li><a class="mybuttontop" href="projects_attack.html">面向计算机视觉的人工智能安全分析</a></li>
                        <li><a class="mybuttonnow">二维/三维图形图像生成、理解与应用</a></li>
                    </ul>
                    <br>

                    <ul class="iconstext">
                        <li><a class="mybuttonlow" href="#sketch">漫画线稿草图生成与应用</a></li>
                        <li><a class="mybuttonlow" href="#editing_and_synthesis">图像编辑与生成</a></li>
                        <li><a class="mybuttonlow" href="#pose">三维姿势估计与动作生成</a></li>
                        <li><a class="mybuttonlow" href="#fashion">服装建模与虚拟试衣</a></li>
                        <li><a class="mybuttonlow" href="#3d_modeling">三维渲染与建模</a></li>
                    </ul>
					
				<!-- Two -->

					<section id="sketch">
						<header class="major">
							<h2 class="head-zh">漫画线稿草图生成与应用</h2>
						</header>

                        <table style="margin-bottom: 0px;" width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                        <tr class="spec">
                            <td width="39%" valign="top" align="center">
                                <img class="image fit thumb" src="./files/images/projects/muten-fast.gif" alt="" style="margin-bottom: 0px;" width="100%" border="0">
                            </td>

                            <td width="17%" valign="top" align="center">
                                <img class="image fit thumb"  src="./files/images/projects/rocket.png" alt="" style="margin-bottom: 0px;" width="100%" border="0">
                                <p class="margin-small">&nbsp;</p>
                                <img src="./files/images/projects/1390_input.png" width="80%">
                            </td>

                            <td width="17%" valign="top" align="center">
                                <img class="image fit thumb"  src="./files/images/projects/rocket-blue-simplest.gif" alt="" style="margin-bottom: 0px;" width="100%" border="0">
                                <p class="margin-small">&nbsp;</p>
                                <img src="./files/images/projects/face-blue-1390-simplest.gif" width="80%">
                            </td>

                            <td width="27%" valign="top" align="center">
                                <img class="image fit thumb"  src="./files/images/projects/color.gif" alt="" style="margin-bottom: 0px;" width="100%" border="0">
                            </td>
                        </tr>
                        </table>

                        <p> 平面草图是人们表达自身想法的最简单快捷的方式之一。与彩色自然图像不同，草图图像仅由黑白两色和稀疏线条构成，
                            因此让计算机去理解草图的内容以及其背后传达的思想是当前一个极具挑战性的问题。
                            目前实验室专注于使用机器学习和深度学习的方法，
                            去解决草图的生成（合成）、理解（比如草图的语义理解、实例分割）
                            及其应用的相关问题（比如草图的自动着色、草图到自然图像的合成等）。
                            相关链接：
                            <br>
                            1. 草图合成相关工作整理汇总（<a href="https://github.com/MarkMoHR/Awesome-Sketch-Synthesis">https://github.com/MarkMoHR/Awesome-Sketch-Synthesis</a>）
                            <br>
                            2. 基于草图的应用相关工作整理汇总（<a href="https://github.com/MarkMoHR/Awesome-Sketch-Based-Applications">https://github.com/MarkMoHR/Awesome-Sketch-Based-Applications</a>）

                        </p>
                        <br>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/pg2022.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://diglib.eg.org/handle/10.2312/pg20221238">Multi-instance Referring Image Segmentation of Scene Sketches based on Global Reference Mechanism</a></h3>
                                    <p>
                                        Peng Ling, Haoran Mo and Chengying Gao<sup>*</sup>
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作提出了一个用于解决基于文本的多实例推断分割单阶段模型。
                                        在该模型中，我们构造了分割模块、文本模块以及全局参照机制模块三部分。
                                        分割模块结合文本模块处理输入的推断文本得到的信息，对草图图像特征进行从粗到细的处理。
                                        同时为了提高模型对图像全局信息的利用，我们提出了全局参照机制模块，使得模型拥有了全局视野，显著增强了模型性能。
                                        除此之外，实验证明本模型还拥有在其他图像域上一定的泛化能力。
                                        <br>
                                        <br>
                                        <em>Pacific Graphics (<strong>PG</strong> 2022) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="./files/PG2022/referring_sketch_segmentation.pdf">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/colorization-PG2021.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="./files/PG2021/line_art_colorization_pg2021_main.pdf">Line Art Colorization Based on Explicit Region Segmentation</a></h3>
                                    <p>
                                        Ruizhi Cao, Haoran Mo and Chengying Gao<sup>*</sup>
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作在全自动草图上色问题中引入分割信息，有效地减少了渗色现象的出现。
                                        此工作主要使用骨架图与区域图作为分割信息，并提出了两种分割信息的融合方式。
                                        分割信息的融合方式被设计为即插即用形式，适用范围十分广泛，
                                        不同的草图上色神经网络模型都能简单而快速地选择合适的融合方式引入，从而显著提升上色效果。
                                        此工作在不同的草图上色模型中证明了两种融合方式的有效性。
                                        <br>
                                        <br>
                                        <em>Computer Graphics Forum (<strong>Pacific Graphics</strong> 2021) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="./files/PG2021/line_art_colorization_pg2021_main.pdf">[论文]</a>
                                        <a href="https://github.com/Ricardo-L-C/ColorizationWithRegion">[代码]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/siggraph2021.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://markmohr.github.io/virtual_sketching/">General Virtual Sketching Framework for Vector Line Art</a></h3>
                                    <p>
                                        Haoran Mo, Edgar Simo-Serra, Chengying Gao<sup>*</sup>, Changqing Zou and Ruomei Wang
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作提出了一个能自动学习将多种类型、任意分辨率的图像直接转化为矢量线稿草图的模型。
                                        此模型基于神经网络结构，模仿人类绘画方式，根据输入图像一笔一划地绘制出草图线条，最后得到矢量输出。
                                        这种以绘画方式生成矢量线稿草图的方法基于本工作提出的动态窗口机制，
                                        即模拟一根虚拟的笔在输入图像上不断移动并同时根据笔触周围的窗口区域情况进行绘制，直至绘制完成。
                                        模型使用了一个可微渲染模块，能够允许仅利用像素图像进行训练学习，而无需依赖矢量数据进行直接监督。
                                        另外，此工作提出了一个笔划正则化的机制去引导模型绘制更少但更长的线条，来使生成的矢量线稿草图更简洁。
                                        大量实验证明本模型能生成高质量的矢量线稿草图，同时具有运行时间更短、泛化能力更强的优点。
                                        <br>
                                        <br>
                                        <em>ACM Transactions on Graphics (<strong>SIGGRAPH</strong> 2021, Journal track) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="https://esslab.jp/publications/HaoranSIGRAPH2021.pdf">[论文]</a>
                                        <a href="https://github.com/MarkMoHR/virtual_sketching">[代码]</a>
                                        <a href="https://markmohr.github.io/virtual_sketching/">[项目主页]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/sketchycoco.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://arxiv.org/abs/2003.02683">SketchyCOCO: Image Generation from Freehand Scene Sketches</a></h3>
                                    <p>
                                        Chengying Gao, Qi Liu, Qi Xu, Jianzhuang Liu, Limin Wang, Changqing Zou*
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        论文首次提出了场景级别的手绘草图生成自然图像的算法。首先根据前背景手绘草图构成特性的不同，
                                        提出了依次生成的解决思路。其次，构造了一个手绘草图及其对应自然图像的场景级别数据集。
                                        最重要的是，针对于实例级别的由手绘草图生成自然图像问题：由于手绘图线条稀疏、表达抽象等特点，
                                        对自然图像的约束性较差，难以学习这二个不同域之间映射关系，本论文构建了一个新的网络架构edgeGAN。
                                        提出了通过隐向量建立起手绘图和边图之间的映射，再由边图对应到自然图像的思路。
                                        具体为利用手绘图和边图都是由线条构成这一特性，采用生成联合图片再编码边图的训练方式，
                                        使得同一向量空间能表达二者的形状、姿势特性，从而构建起手绘图和边图的隐式对应关系。
                                        再借助边图与自然图像的强约束性，使得生成的自然图像合理且符合输入的手绘图。
                                        通过与现有方法大量的对比实验证明，该方法无论在实例或是场景级别上都优于目前最先进的方法。
                                        <br>
                                        <br>
                                        <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>, 2020) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="https://arxiv.org/abs/2003.02683">[论文]</a>
                                        <a href="https://github.com/sysu-imsl/SketchyCOCO">[代码]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/colorization2.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://github.com/SketchyScene/SketchySceneColorization">Language-based Colorization of Scene Sketches</a></h3>
                                    <p>
                                        Changqing Zou<sup>#</sup>, Haoran Mo<sup>#</sup>(joint first author), Chengying Gao<sup>*</sup>, Ruofei Du and Hongbo Fu
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作首次提出用户通过语言指令的人机交互方式实现场景草图自动涂色，
                                        并在此基础上设计与构建了场景级草图自动涂色系统。同时，此工作也首次提出并实现针对场景草图，
                                        基于语言指令的指定目标多实例分割算法，为场景级草图的理解提供了一种有效的解决方法。
                                        <br>
                                        <br>
                                        <em>ACM Transactions on Graphics (<strong>SIGGRAPH Asia</strong> 2019, Journal track) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="http://mo-haoran.com/files/SIGA19/SketchColorization_paper_SA2019.pdf">[论文]</a>
                                        <a href="https://github.com/SketchyScene/SketchySceneColorization">[代码]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/sketchyscene.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://sketchyscene.github.io/SketchyScene/">SketchyScene: Richly-Annotated Scene Sketches</a></h3>
                                    <p>
                                        Changqing Zou<sup>#</sup>, Qian Yu<sup>#</sup>, Ruofei Du, Haoran Mo, Yi-Zhe Song, Tao Xiang, Chengying Gao, Baoquan Chen<sup>*</sup>, and Hao Zhang
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作构建了一个大规模的场景草图数据集，并改进现有模型架构来进行语义理解分割实验，作为研究基准。
                                        同时，此工作提出一种专门用于草图数据的背景忽略训练策略，能大幅度提升神经网络对草图的理解能力。
                                        <br>
                                        <br>
                                        <em>European Conference on Computer Vision (<strong>ECCV</strong>, 2018) &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper.pdf">[论文]</a>
                                        <a href="https://github.com/SketchyScene/SketchyScene">[代码]</a>
                                    </p>
                                </td>
                            </tr>

                        </table>


					</section>

                    <div align="right">
                        <a href="#">Back to top</a>
                    </div>

                    <section id="editing_and_synthesis">
						<header class="major">
							<h2 class="head-zh">图像编辑与生成</h2>
						</header>

                        <p>
                            实验室关注图像修复、颜色复原、颜色迁移和图像非真实感渲染等图像编辑与生成研究方向。
                            <p class="margin-small">&nbsp;</p>
                            <strong>图像修复</strong>：指将图像中缺失的部分进行自动补全。
                            目前实验室专注于使用深度学习的方式提出图像修复算法。
                            <br>
                            <strong>颜色复原</strong>：指将灰度图像自动转化为彩色图像。当前颜色复原问题受到很多研究者的关注，因为其应用场景很广，
                            比如可以将黑白老照片、年代久远的黑白电影转化为带有颜色的影像，使经典作品以更生动的形象展现在我们面前。
                            目前实验室专注于使用深度学习的方式从大量数据中学习颜色转化的方法。
                            <br>
                            <strong>颜色迁移</strong>：指将目标颜色色调迁移到给定的原始图像，同时保持整体图像的合理性。
                            颜色迁移问题能应用到艺术创作等领域中。
                            实验室目前专注于使用数字图像处理和数据统计分析的混合方法来解决此问题。
                            <br>
                            <strong>图像非真实感渲染</strong>：非真实感渲染旨在利用计算机模拟现实中存在的各种不同艺术形式的绘制风格（铅笔、水彩、卡通、国画和油画等）。
                            实验室专注于研究结合传统方法与深度学习方法，探索各种不同艺术形式的非真实感渲染算法。
                            其中，彩铅画是一种特殊形式的非真实感渲染方式，如何准确地表达出彩铅画中独特的色彩特性和纹理效果，
                            从而使生成出的风格兼顾真实感与美观性，是一个具有较大挑战性的问题。
                            <p class="margin-small">&nbsp;</p>
                            相关链接：图像上色、颜色复原相关工作整理汇总（<a href="https://github.com/MarkMoHR/Awesome-Image-Colorization">https://github.com/MarkMoHR/Awesome-Image-Colorization</a>）

                        </p>
                        <br>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/cvpr2023.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://arxiv.org/abs/2303.17867">CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer</a></h3>
                                    <p>
                                        Linfeng Wen, Chengying Gao*, Changqing Zou
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作提出一种可逆的风格迁移框架保持内容亲和力。该框架使用基于Channel Refinement模块、无冗余信息的可逆残差网络实现图像-特征空间的双向映射，
                                        结合无偏线性变换模块cWCT对齐风格信息，并通过Matting Laplacian训练损失保持像素亲和力。
                                        本方法通过实验证明该可逆框架能实现内容亲和力的保持、风格化的一致性，在图像、视频处理上优于现有方法。
                                        <br>
                                        <br>
                                        <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>, 2023) &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="https://arxiv.org/abs/2303.17867">[论文]</a>
                                        <a href="https://github.com/linfengWen98/CAP-VSTNet">[代码]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/icme2021inpainting.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://ieeexplore.ieee.org/abstract/document/9428367">Structural Prior Guided Image Inpainting for Complex Scene</a></h3>
                                    <p>
                                        Shuxin Wei, Chengying Gao
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        现有基于深度学习的图像修复方法在面对上下文信息丰富的小面积破损区域已取得较好的修复效果，
                                        然而对复杂场景的大面积破损区域进行修复时往往会产生语义失真、边缘模糊等问题。
                                        本文将复杂场景的图像修复问题分解为语义分割图修复与语义分割图引导的纹理信息修复两阶段，
                                        通过特征相关矩阵评估语义分割图与破损图像已知区域之间的相关性，从而完成破损区域的纹理生成。
                                        <br>
                                        <br>
                                        <em>International Conference on Multimedia & Expo (<strong>ICME</strong>, 2021)  <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://ieeexplore.ieee.org/abstract/document/9428367">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/jisuanjixuebao19.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="">基于稀疏结构的复杂物体修复</a></h3>
                                    <p>
                                        高成英，徐仙儿，罗燕媚，王栋
                                        <br>
                                        <br>
                                        <em>计算机学报</em>，2019
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/Neurocomputing18.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://www.sciencedirect.com/science/article/pii/S0925231218306672">An edge-refined vectorized deep colorization model for grayscale-to-color images</a></h3>
                                    <p>
                                        Zhuo Su, Xiangguo Liang, Jiaming Guo, Chengying Gao, Xiaonan Luo
                                        <br>
                                        <br>
                                        <em>Neurocomputing</em>, 2018
                                        <br>
                                        <a href="https://www.sciencedirect.com/science/article/pii/S0925231218306672">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/pencil-art.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%" class="tdp">
								    <h3> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13334">PencilArt: A Chromatic Penciling Style Generation Framework</a></h3>
                                    <p>
                                        Chengying Gao, Mengyue Tang, Xiangguo Liang, Zhou Su, Changqing Zou
                                        <br>
                                        <br>
                                        <em>Computer Graphics Forum (<strong>CGF</strong>), 2018 &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13334">[论文]</a>
                                    </p>
                                </td>
                            </tr>
                            
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/spic17.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://www.sciencedirect.com/science/article/pii/S0923596517300772">Data-Driven Image Completion for Complex Object</a></h3>
                                    <p>
                                        Chengying Gao, Yanmei Luo, Hefeng Wu*, Dong Wang
                                        <br>
                                        <br>
                                        <em>Signal Processing: Image Communication</em>, 2017
                                        <br>
                                        <a href="https://www.sciencedirect.com/science/article/pii/S0923596517300772">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/color-transfer.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://changqingzou.weebly.com/uploads/5/9/3/6/59369643/wang_et_al-2017-computer_graphics_forum-min.pdf">L0 Gradient-Preserving Color Transfer</a></h3>
                                    <p>
                                        Dong Wang, Changqing Zou, Guiqing Li, Chengying Gao, Zhuo Su, Ping Tan
                                        <br>
                                        <br>
                                        <em>Computer Graphics Forum (<strong>CGF</strong>), 2017 &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://changqingzou.weebly.com/uploads/5/9/3/6/59369643/wang_et_al-2017-computer_graphics_forum-min.pdf">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                        </table>

					</section>

                    <div align="right">
                        <a href="#">Back to top</a>
                    </div>

                    <section id="pose">
						<header class="major">
							<h2 class="head-zh">三维姿势估计与动作生成</h2>
						</header>

                        <p>
                            实验室关注三维姿势、手势估计和三维动作生成等研究方向。
                        </p>
                        <br>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/icme2022-4.gif" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="./files/ICME2022/Unpaired_Motion_Style_Transfer_camera_ready.pdf">Unpaired Motion Style Transfer with Motion-oriented Projection Flow Network</a></h3>
                                    <p>	Yue Huang, Haoran Mo, Xiao Liang, Chengying Gao*
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        本文提出了一种基于投影流网络和自适应实例归一化的不成对动作风格迁移方法，
                                        利用可逆的投影流网络来投影和还原动作特征，利用无偏的自适应实例归一化来生成风格化特征。
                                        根据动作数据的时序性特点，本文进一步设计了插值模块和引入Transformer的加性耦合层，
                                        以有效提升模型对风格的归纳能力和动作的真实性。
                                        实验表明，该方法可以在保留完整内容的情况下有效迁移动作的风格。
                                        相比现有的的不成对动作风格迁移方法，该模型泛化能力更强，在不可见风格上有更好的聚类效果。
                                        <br>
                                        <br>
                                        <em>International Conference on Multimedia & Expo (<strong>ICME</strong>, 2022) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="./files/ICME2022/Unpaired_Motion_Style_Transfer_camera_ready.pdf">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/neurocomputing2022.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://doi.org/10.1016/j.neucom.2021.12.013">3D interacting hand pose and shape estimation from a single RGB image</a></h3>
                                    <p>	Chengying Gao*, Yujia Yang, Wensheng Li
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作提出了一种基于RGB图像的双手网格重建算法。该算法采用分组卷积来分别提取左右特征，
                                        有效避免了左右手特征互相干扰。此外，本工作还提出了多特征融合模块MF-block，该模块结合了图像特征，
                                        姿势特征和上采样特征，能有效预测被遮挡部分的2D关键点。
                                        最后，本工作提出了基于Transformer机制的3D网格生成模型。
                                        <br>
                                        <br>
                                        <em>Neurocomputing, 2022</em>
                                        <br>
                                        <a href="https://doi.org/10.1016/j.neucom.2021.12.013">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                        </table>
					</section>

                    <div align="right">
                        <a href="#">Back to top</a>
                    </div>

                    <section id="fashion">
                        <header class="major">
							<h2 class="head-zh">服装建模与虚拟试衣</h2>
						</header>

                        <p> 三维服装建模和虚拟试衣在服装制造、影视娱乐和虚拟现实等领域具有广泛的应用，引起了国内外学者的广泛关注。
                            然而由于服装材料、款式及人体体型的多样性和复杂性等问题，目前仍面临着巨大的挑战。
                            近年来实验室专注于服装设计过程中的快速建模和2D、3D的虚拟服装展示算法，
                            并在虚拟试衣和三维服装建模方面取得一些研究成果。

                        </p>
                        <br>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/fashionGAN.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13552">FashionGAN: Display your fashion design using Conditional Generative Adversarial Nets</a></h3>
                                    <p>	Yirui Cui, Qi Liu, Chengying Gao*, Zhuo Su
                                        <br>
                                        <br>
                                        <em>Computer Graphics Forum (<strong>Pacific Graphics</strong>, 2018) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13552">[论文]</a>
                                        <a href="https://github.com/Cuiyirui/FashionGAN">[代码]</a>
										<a href="https://drive.google.com/drive/folders/1DACqCXlJRQxRysO6RVNO7vOoR8YzrjTQ?usp=sharing">[数据集]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/pcm18.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://link.springer.com/chapter/10.1007%2F978-3-030-00776-8_25">Automatic 3D Garment Fitting Based on Skeleton Driving</a></h3>
                                    <p> Haozhong Cai, Guangyuan Shi, Chengying Gao*, Dong Wang
                                        <br>
                                        <br>
                                        <em>Pacific-Rim Conference on Multimedia (<strong>PCM</strong>, 2018) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-C)</font></strong></em>
                                        <br>
                                        <a href="https://link.springer.com/chapter/10.1007%2F978-3-030-00776-8_25">[论文]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/pg14.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://diglib.eg.org/handle/10.2312/pgs.20141250.043-048">Automatic Garment Modeling From Front And Back Images</a></h3>
                                    <p>	Lifeng Huang, Chengying Gao*
                                        <br>
                                        <br>
                                        <em>Pacific Graphics (<strong>PG</strong>, 2014) &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://diglib.eg.org/handle/10.2312/pgs.20141250.043-048">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                        </table>

                    </section>

                    <div align="right">
                        <a href="#">Back to top</a>
                    </div>

					<section id="3d_modeling">
                        <header class="major">
							<h2 class="head-zh">三维渲染与建模</h2>
						</header>
                        <p>
                            实验室关注基于窄带的快速流体表面重建和织物建模与渲染等三维渲染与建模研究方向。
                            <p class="margin-small">&nbsp;</p>
                            <strong>基于窄带的快速流体表面重建</strong>：流体表面网格的重建介于流体模拟和流体渲染之间，
                            重建出来的流体网格质量与最终渲染效果有着极其密切的关系。
							在当前有限的算力前提下，如何在尽可能保证网格质量的同时加快重建效率是一个非常具有挑战性的难题。
                            <br>
                            <strong>织物建模与渲染</strong>：织物是虚拟世界中不可或缺的一部分。
                            基于纤维的纱线几何结构可以极大地还原织物种类的多样性和结构的复杂性，对织物的真实感渲染起着至关重要的作用。
                            然而纤维带来大量的微观细节，这使得庞大的织物模型只能在动画制作、电影特效等离线渲染领域中得以运用，
                            而无法满足电子游戏等实时渲染的需求。近年来实验室专注于找到一种高质量、全自动的三角网格模型到纱线模型的转换算法，
                            进而使用基于微观模型的织物实时渲染算法对纱线模型进行展示，以模拟出织物纤维级别的细节特征。

                        </p>
                        <br>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/cgi2020.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%" class="tdp">
								    <h3> <a href="https://link.springer.com/article/10.1007/s00371-020-01898-2">A Completely Parallel Surface Reconstruction Method for Particle-Based Fluids</a></h3>
                                    <p>
                                        Wencong Yang, Chengying Gao
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        本文首先提出了一种快速、简单、极其准确的流体表面的窄带方法，使得表面重建算法（例如Marching Cube）
										能够准确地定位到有效的流体表面区域，极大地避免了无用的计算过程。
										同时，我们分析了重建过程潜在的数据竞争和条件分支，利用互斥前缀和算法，
										将整个流体表面重建的过程完全并行化，大大加快了表面重建的效率。
                                        <br>
                                        <br>
                                        <em>Computer Graphics International (<strong>CGI</strong>, 2020) &nbsp;<strong><font color="#FF0000">(CCF-C)</font></strong></em>
                                        <br>
                                        <a href="https://link.springer.com/article/10.1007/s00371-020-01898-2">[论文]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/fabric-model.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%" class="tdp">
								    <h3> <a href="./files/3D-materials/fabric-model.pdf">全自动的纱线模型生成算法</a></h3>
                                    <p>
										张泽坤
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        算法可以自动地将三角网格模型转化为纱线模型。
                                        但算法存在模型适用范围较小、特定类型纱线模型质量较差等缺点，目前正在研究如何解决这些问题。
                                        若问题得以解决，有望改进、简化当前织物工业化生产的设计流程，提高生产效率。
                                        <br>
                                        <br>
                                        <a href="./files/3D-materials/fabric-model.pdf">[详细介绍 (PPT)]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/fabric-render.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%" class="tdp">
								    <h3> <a href="./files/3D-materials/fabric-render.pdf">基于微观模型的织物实时渲染算法</a></h3>
                                    <p>
										罗幸荣
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        纤维级织物，是指在纤维级微观尺度下进行建模的织物。
                                        它将织物表示成大量纤维的集合，显式地描述了纤维、单纱以及纱线的相对几何关系。
                                        算法使用核心纤维面片来对单纱中规律缠绕的纤维进行替代，从而减小实时光栅化带来的性能开销，
                                        并与常规纤维相结合，使得能够在可交互的帧率下对纤维级织物进行渲染。
                                        <br>
                                        <br>
                                        <a href="./files/3D-materials/fabric-render.pdf">[详细介绍 (PPT)]</a>
                                    </p>
                                </td>
                            </tr>

                        </table>


                    </section>

                    <div align="right">
                        <a href="#">Back to top</a>
                        <br>
                        <p> &nbsp; </p>
                    </div>

				</div>
			</div>
			
			<!-- Sidebar -->
				<div id="sidebar">
					<div class="inner">

						<!-- Search -->
							<section id="search" class="alt">
								<!--<form method="post" action="#">-->
									<!--<input type="text" name="query" id="query" placeholder="Search" />-->
								<!--</form>-->
                                <div style="text-align:center" >
                                    <img src="./files/images/logo2.png" alt="" width="100%"/>
                                </div>
							</section>

						<!-- Menu -->
							<nav id="menu">
								<header class="major">
									<h2>Menu</h2>
								</header>
								<ul>
									<li><a class="menu-zh" href="index.html">主页</a></li>
									<li><a class="menu-zh" href="projects_indoor.html">研究项目</a></li>
									<li><a class="menu-zh" href="publications_papers.html">论文发表</a></li>
									<li><a class="menu-zh" href="members.html">成员信息</a></li>
									<li><a class="menu-zh" href="students_dev.html">学生发展</a></li>
									<li><a class="menu-zh" href="materials_datasets.html">相关资料</a></li>
									<!--
									<li>
										<span class="opener">Submenu</span>
										<ul>
											<li><a href="#">Lorem Dolor</a></li>
											<li><a href="#">Ipsum Adipiscing</a></li>
											<li><a href="#">Tempus Magna</a></li>
											<li><a href="#">Feugiat Veroeros</a></li>
										</ul>
									</li>
									<li><a href="#">Etiam Dolore</a></li>
									<li><a href="#">Adipiscing</a></li>
									<li>
										<span class="opener">Another Submenu</span>
										<ul>
											<li><a href="#">Lorem Dolor</a></li>
											<li><a href="#">Ipsum Adipiscing</a></li>
											<li><a href="#">Tempus Magna</a></li>
											<li><a href="#">Feugiat Veroeros</a></li>
										</ul>
									</li>
									-->
								</ul>
							</nav>

						<!-- Section -->
						<!--
							<section>
								<header class="major">
									<h2>Ante interdum</h2>
								</header>
								<div class="mini-posts">
									<article>
										<a href="#" class="image"><img src="./images/pic07.jpg" alt="" /></a>
										<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
									</article>
									<article>
										<a href="#" class="image"><img src="./images/pic08.jpg" alt="" /></a>
										<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
									</article>
									<article>
										<a href="#" class="image"><img src="./images/pic09.jpg" alt="" /></a>
										<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
									</article>
								</div>
								<ul class="actions">
									<li><a href="#" class="button">More</a></li>
								</ul>
							</section>
                        -->
						<!-- Section -->
							<section>
								<header class="major">
									<h2 class="head-zh">联系方式</h2>
								</header>
								<ul class="contact">
									<li class="fa-envelope-o">联系邮箱: <a href="#">sysuimsl@126.com</a></li>
									<li class="fa-home">地址: <a href="https://goo.gl/maps/P7iu1XtZfrk5TsKz5">中国 广州, 中山大学, 计算机学院, A409室</a>
									</li>
								</ul>
								<!--<article class="5u 10u$(xsmall) work-item">-->
										<!--<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&d=mhnrYabZI2bz_eHk1W_A8VvNxtAjYBrWfIfxbLnTRPQ&co=4c5459&cmo=faa659'></script>-->
								<!--</article>-->
								<!--<a href="https://info.flagcounter.com/Dsap"><img src="https://s11.flagcounter.com/count2/Dsap/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>-->
							</section>

						<!-- Footer -->
							<footer id="footer">
								<p class="copyright">
									<li>Copyright &copy; 2019-2023 SYSU-IMSL</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</footer>

					</div>
				</div>
        <!-- push the page to Baidu -->
		<script>
		(function(){
		    var bp = document.createElement('script');
		    var curProtocol = window.location.protocol.split(':')[0];
		    if (curProtocol === 'https'){
		   bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
		  }
		  else{
		  bp.src = 'http://push.zhanzhang.baidu.com/push.js';
		  }
		    var s = document.getElementsByTagName("script")[0];
		    s.parentNode.insertBefore(bp, s);
		})();
		</script>
		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
	</body>
</html>

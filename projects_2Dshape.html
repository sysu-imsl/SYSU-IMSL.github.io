<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>智能与多媒体科学实验室</title>
		<link rel="icon" href="./files/images/logo.png">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<meta name="sysuimsl" content="智能与多媒体科学实验室" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main-v20210707.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
		<script>
		  (adsbygoogle = window.adsbygoogle || []).push({
		    google_ad_client: "ca-pub-6919411062262434",
		    enable_page_level_ads: true
		  });
		</script>
	</head>
	<script src="//instant.page/1.1.0" type="module" integrity="sha384-EwBObn5QAxP8f09iemwAJljc+sU+eUXeL9vSBw1eNmVarwhKk2F9vBEpaN9rsrtp"></script>
	<body class="is-preload">
	    <div id="wrapper">
		<!-- Main -->
			<div id="main">
		    	<div class="inner">
					<!-- Header -->
					<header id="header">
						<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                            <tr class="spec">
                                <td width="70%">
									<h11 class="head-zh">智能与多媒体科学实验室</h11>
                                </td>
                                <td width="30%">
								    <ul class="icons">
										<li><a href="https://github.com/sysu-imsl" class="icon fa-github"><span class="label">Github</span></a></li>
										<li><a href="mailto:sysuimsl@126.com" class="icon fa-envelope-o"><span class="label">Email</span></a></li>
										<li><a href="en/projects_2Dshape.html">Eng. Version ☞</a></li>
									</ul>
                                </td>
                            </tr>
                        </table>
					</header>

                    <br>


                    <!--<h2>-->
                        <!--<ul class="icons">-->
							<!--<li><a href="projects_indoor_en.html">test</a></li>-->
							<!--<li><a href="projects_indoor_en.html">test1</a></li>-->
                            <!--<li><a href="projects_indoor_en.html">test3</a></li>-->
                        <!--</ul>-->
                    <!--</h2>-->

                    <ul class="iconstext">
                        <li><a class="mybuttontop" href="projects_indoor.html">室内空间位置智能感知与应用</a></li>
                        <li><a class="mybuttontop" href="projects_attack.html">面向计算机视觉的人工智能安全分析</a></li>
                        <li><a class="mybuttonnow">二维/三维图形图像生成、理解与应用</a></li>
                    </ul>
                    <br>

                    <ul class="iconstext">
                        <li><a class="mybuttonlow" href="#sketch">漫画线稿草图生成与应用</a></li>
                        <li><a class="mybuttonlow" href="#editing_and_synthesis">图像编辑与生成</a></li>
                        <li><a class="mybuttonlow" href="#pose">三维姿势估计与动作生成</a></li>
                        <li><a class="mybuttonlow" href="#fashion">服装建模与虚拟试衣</a></li>
                        <li><a class="mybuttonlow" href="#3d_modeling">多媒体处理与三维渲染建模</a></li>
                    </ul>
					
				<!-- Two -->

					<section id="sketch">
						<header class="major">
							<h2 class="head-zh">漫画线稿草图生成与应用</h2>
						</header>

                        <table style="margin-bottom: 0px;" width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                        <tr class="spec">
                            <td width="39%" valign="top" align="center">
                                <img class="image fit thumb" src="./files/images/projects/muten-fast.gif" alt="" style="margin-bottom: 0px;" width="100%" border="0">
                            </td>

                            <td width="17%" valign="top" align="center">
                                <img class="image fit thumb"  src="./files/images/projects/rocket.png" alt="" style="margin-bottom: 0px;" width="100%" border="0">
                                <p class="margin-small">&nbsp;</p>
                                <img src="./files/images/projects/1390_input.png" width="80%">
                            </td>

                            <td width="17%" valign="top" align="center">
                                <img class="image fit thumb"  src="./files/images/projects/rocket-blue-simplest.gif" alt="" style="margin-bottom: 0px;" width="100%" border="0">
                                <p class="margin-small">&nbsp;</p>
                                <img src="./files/images/projects/face-blue-1390-simplest.gif" width="80%">
                            </td>

                            <td width="27%" valign="top" align="center">
                                <img class="image fit thumb"  src="./files/images/projects/color.gif" alt="" style="margin-bottom: 0px;" width="100%" border="0">
                            </td>
                        </tr>
                        </table>

                        <p> 平面草图是人们表达自身想法的最简单快捷的方式之一。与彩色自然图像不同，草图图像仅由黑白两色和稀疏线条构成，
                            因此让计算机去理解草图的内容以及其背后传达的思想是当前一个极具挑战性的问题。
                            目前实验室专注于使用机器学习和深度学习的方法，
                            去解决草图的生成（合成）、理解（比如草图的语义理解、实例分割）
                            及其应用的相关问题（比如草图的自动着色、草图到自然图像的合成等）。
                            相关链接：
                            <br>
                            1. 草图合成相关工作整理汇总（<a href="https://github.com/MarkMoHR/Awesome-Sketch-Synthesis">https://github.com/MarkMoHR/Awesome-Sketch-Synthesis</a>）
                            <br>
                            2. 基于草图的应用相关工作整理汇总（<a href="https://github.com/MarkMoHR/Awesome-Sketch-Based-Applications">https://github.com/MarkMoHR/Awesome-Sketch-Based-Applications</a>）

                        </p>
                        <br>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/tog2024.gif" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://markmohr.github.io/JoSTC/">Joint Stroke Tracing and Correspondence for 2D Animation</a></h3>
                                    <p>
                                        Haoran Mo, Chengying Gao<sup>*</sup> and Ruomei Wang
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作提出一个基于草图笔画矢量化和关联匹配的二维动画生成方法。
                                        该方法以像素草图关键帧序列与人工标注的首帧矢量草图为输入，实现为后续关键帧生成矢量笔画，并构成帧间笔画匹配关系，用于后续动画生成。
                                        虽然以干净草图进行训练，模型可以泛化应用于粗糙草图。
                                        生成的结果能够直接导入插帧软件，进行连贯序列帧的生成，构成二维动画。
                                        本方法还提出一个自适应的空间变换模块（ASTM），用于处理非刚性运动和笔画扭曲。
                                        另外，本工作还构建了一个包含10k+组像素关键帧及其带有笔画匹配标注的矢量图构成的数据集，用于模型的训练。
                                        <br>
                                        <br>
                                        <em>ACM Transactions on Graphics (Presented at <strong>SIGGRAPH 2024</strong>) &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="./files/TOG2024/SketchTracing_TOG2024_personal.pdf">[论文]</a>
                                        <a href="https://github.com/MarkMoHR/JoSTC">[代码]</a>
                                        <a href="https://markmohr.github.io/JoSTC/">[项目主页]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/icme24-editing.gif" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://github.com/MarkMoHR/DiffSketchEdit">Text-based Vector Sketch Editing with Image Editing Diffusion Prior</a></h3>
                                    <p>
                                        Haoran Mo, Xusheng Lin, Chengying Gao<sup>*</sup> and Ruomei Wang
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        本文提出了一个基于文本的矢量草图编辑框架，来提高图形设计的效率。该方法的关键思想是提取像素级图像编辑扩散模型的先验知识，并迁移到矢量草图编辑任务上。
                                        该框架支持3种编辑模式，并允许进行迭代式编辑。为了满足仅调整目标区域，而不改动其他笔画的需求，本文设计了一个局部区域笔画编辑机制，能够自动预测反映可编辑区域的编辑掩膜，并仅对掩膜区域内的笔画进行改动。
                                        <br>
                                        <br>
                                        <em>International Conference on Multimedia & Expo (<strong>ICME</strong>, 2024) &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="./files/ICME2024/ICME2024_sketch_editing_final.pdf">[论文]</a>
                                        <a href="https://github.com/MarkMoHR/DiffSketchEdit">[代码]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/icme24-animation.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="">Video-Driven Sketch Animation via Cyclic Reconstruction Mechanism</a></h3>
                                    <p>
                                        Zhuo Xie, Haoran Mo and Chengying Gao<sup>*</sup>
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        针对二维草图动画制作需要耗费大量时间绘制关键帧的问题，本文提出了一种将视频作为参考信息，实现草图图像动画生成的自动化解决方案。该方案包括从驱动视频中提取动作、在保持源草图外观特征的前提下将视频动作注入静态草图中，以产生动态的草图序列。为减少复杂运动导致的模糊伪影并保持笔划线条的连贯性，本文将草图的内部掩码图作为显式的引导信息，用于指示运动主体的内部区域并确保各运动部位的完整性。此外，为了在建立动作模型时弥合驱动帧和草图之间的域差距，本文引入一种循环重建机制，以增加动作模型对不同数据域的兼容能力，从而提高草图动画和驱动视频之间的动作一致性。定量与定性实验结果，充分验证了本文方法相比于现有图像动画方法的优越性。
                                        <br>
                                        <br>
                                        <em>International Conference on Multimedia & Expo (<strong>ICME</strong>, 2024) &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="./files/ICME2024/ICME2024_sketch_animation_final.pdf">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/pg2022.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://diglib.eg.org/handle/10.2312/pg20221238">Multi-instance Referring Image Segmentation of Scene Sketches based on Global Reference Mechanism</a></h3>
                                    <p>
                                        Peng Ling, Haoran Mo and Chengying Gao<sup>*</sup>
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作提出了一个用于解决基于文本的多实例推断分割单阶段模型。
                                        在该模型中，我们构造了分割模块、文本模块以及全局参照机制模块三部分。
                                        分割模块结合文本模块处理输入的推断文本得到的信息，对草图图像特征进行从粗到细的处理。
                                        同时为了提高模型对图像全局信息的利用，我们提出了全局参照机制模块，使得模型拥有了全局视野，显著增强了模型性能。
                                        除此之外，实验证明本模型还拥有在其他图像域上一定的泛化能力。
                                        <br>
                                        <br>
                                        <em>Pacific Graphics (<strong>PG</strong> 2022) &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="./files/PG2022/referring_sketch_segmentation.pdf">[论文]</a>
                                        <a href="https://github.com/LQY404/GRM-Net">[代码]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/colorization-PG2021.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="./files/PG2021/line_art_colorization_pg2021_main.pdf">Line Art Colorization Based on Explicit Region Segmentation</a></h3>
                                    <p>
                                        Ruizhi Cao, Haoran Mo and Chengying Gao<sup>*</sup>
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作在全自动草图上色问题中引入分割信息，有效地减少了渗色现象的出现。
                                        此工作主要使用骨架图与区域图作为分割信息，并提出了两种分割信息的融合方式。
                                        分割信息的融合方式被设计为即插即用形式，适用范围十分广泛，
                                        不同的草图上色神经网络模型都能简单而快速地选择合适的融合方式引入，从而显著提升上色效果。
                                        此工作在不同的草图上色模型中证明了两种融合方式的有效性。
                                        <br>
                                        <br>
                                        <em>Computer Graphics Forum (<strong>Pacific Graphics</strong> 2021) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="./files/PG2021/line_art_colorization_pg2021_main.pdf">[论文]</a>
                                        <a href="https://github.com/Ricardo-L-C/ColorizationWithRegion">[代码]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/siggraph2021.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://markmohr.github.io/virtual_sketching/">General Virtual Sketching Framework for Vector Line Art</a></h3>
                                    <p>
                                        Haoran Mo, Edgar Simo-Serra, Chengying Gao<sup>*</sup>, Changqing Zou and Ruomei Wang
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作提出了一个能自动学习将多种类型、任意分辨率的图像直接转化为矢量线稿草图的模型。
                                        此模型基于神经网络结构，模仿人类绘画方式，根据输入图像一笔一划地绘制出草图线条，最后得到矢量输出。
                                        这种以绘画方式生成矢量线稿草图的方法基于本工作提出的动态窗口机制，
                                        即模拟一根虚拟的笔在输入图像上不断移动并同时根据笔触周围的窗口区域情况进行绘制，直至绘制完成。
                                        模型使用了一个可微渲染模块，能够允许仅利用像素图像进行训练学习，而无需依赖矢量数据进行直接监督。
                                        另外，此工作提出了一个笔画正则化的机制去引导模型绘制更少但更长的线条，来使生成的矢量线稿草图更简洁。
                                        大量实验证明本模型能生成高质量的矢量线稿草图，同时具有运行时间更短、泛化能力更强的优点。
                                        <br>
                                        <br>
                                        <em>ACM Transactions on Graphics (<strong>SIGGRAPH</strong> 2021, Journal track) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="https://esslab.jp/publications/HaoranSIGRAPH2021.pdf">[论文]</a>
                                        <a href="https://github.com/MarkMoHR/virtual_sketching">[代码]</a>
                                        <a href="https://markmohr.github.io/virtual_sketching/">[项目主页]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/sketchycoco.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://arxiv.org/abs/2003.02683">SketchyCOCO: Image Generation from Freehand Scene Sketches</a></h3>
                                    <p>
                                        Chengying Gao, Qi Liu, Qi Xu, Jianzhuang Liu, Limin Wang, Changqing Zou*
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        论文首次提出了场景级别的手绘草图生成自然图像的算法。首先根据前背景手绘草图构成特性的不同，
                                        提出了依次生成的解决思路。其次，构造了一个手绘草图及其对应自然图像的场景级别数据集。
                                        最重要的是，针对于实例级别的由手绘草图生成自然图像问题：由于手绘图线条稀疏、表达抽象等特点，
                                        对自然图像的约束性较差，难以学习这二个不同域之间映射关系，本论文构建了一个新的网络架构edgeGAN。
                                        提出了通过隐向量建立起手绘图和边图之间的映射，再由边图对应到自然图像的思路。
                                        具体为利用手绘图和边图都是由线条构成这一特性，采用生成联合图片再编码边图的训练方式，
                                        使得同一向量空间能表达二者的形状、姿势特性，从而构建起手绘图和边图的隐式对应关系。
                                        再借助边图与自然图像的强约束性，使得生成的自然图像合理且符合输入的手绘图。
                                        通过与现有方法大量的对比实验证明，该方法无论在实例或是场景级别上都优于目前最先进的方法。
                                        <br>
                                        <br>
                                        <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>, 2020) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="https://arxiv.org/abs/2003.02683">[论文]</a>
                                        <a href="https://github.com/sysu-imsl/SketchyCOCO">[代码]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/colorization2.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://github.com/SketchyScene/SketchySceneColorization">Language-based Colorization of Scene Sketches</a></h3>
                                    <p>
                                        Changqing Zou<sup>#</sup>, Haoran Mo<sup>#</sup>(joint first author), Chengying Gao<sup>*</sup>, Ruofei Du and Hongbo Fu
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作首次提出用户通过语言指令的人机交互方式实现场景草图自动涂色，
                                        并在此基础上设计与构建了场景级草图自动涂色系统。同时，此工作也首次提出并实现针对场景草图，
                                        基于语言指令的指定目标多实例分割算法，为场景级草图的理解提供了一种有效的解决方法。
                                        <br>
                                        <br>
                                        <em>ACM Transactions on Graphics (<strong>SIGGRAPH Asia</strong> 2019, Journal track) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="http://mo-haoran.com/files/SIGA19/SketchColorization_paper_SA2019.pdf">[论文]</a>
                                        <a href="https://github.com/SketchyScene/SketchySceneColorization">[代码]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/sketchyscene.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://sketchyscene.github.io/SketchyScene/">SketchyScene: Richly-Annotated Scene Sketches</a></h3>
                                    <p>
                                        Changqing Zou<sup>#</sup>, Qian Yu<sup>#</sup>, Ruofei Du, Haoran Mo, Yi-Zhe Song, Tao Xiang, Chengying Gao, Baoquan Chen<sup>*</sup>, and Hao Zhang
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作构建了一个大规模的场景草图数据集，并改进现有模型架构来进行语义理解分割实验，作为研究基准。
                                        同时，此工作提出一种专门用于草图数据的背景忽略训练策略，能大幅度提升神经网络对草图的理解能力。
                                        <br>
                                        <br>
                                        <em>European Conference on Computer Vision (<strong>ECCV</strong>, 2018) &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper.pdf">[论文]</a>
                                        <a href="https://github.com/SketchyScene/SketchyScene">[代码]</a>
                                    </p>
                                </td>
                            </tr>

                        </table>


					</section>

                    <div align="right">
                        <a href="#">Back to top</a>
                    </div>

                    <section id="editing_and_synthesis">
						<header class="major">
							<h2 class="head-zh">图像编辑与生成</h2>
						</header>

                        <p>
                            实验室关注图像修复、颜色复原、颜色迁移和图像非真实感渲染等图像编辑与生成研究方向。
                            <p class="margin-small">&nbsp;</p>
                            <strong>图像修复</strong>：指将图像中缺失的部分进行自动补全。
                            目前实验室专注于使用深度学习的方式提出图像修复算法。
                            <br>
                            <strong>颜色复原</strong>：指将灰度图像自动转化为彩色图像。当前颜色复原问题受到很多研究者的关注，因为其应用场景很广，
                            比如可以将黑白老照片、年代久远的黑白电影转化为带有颜色的影像，使经典作品以更生动的形象展现在我们面前。
                            目前实验室专注于使用深度学习的方式从大量数据中学习颜色转化的方法。
                            <br>
                            <strong>颜色迁移</strong>：指将目标颜色色调迁移到给定的原始图像，同时保持整体图像的合理性。
                            颜色迁移问题能应用到艺术创作等领域中。
                            实验室目前专注于使用数字图像处理和数据统计分析的混合方法来解决此问题。
                            <br>
                            <strong>图像非真实感渲染</strong>：非真实感渲染旨在利用计算机模拟现实中存在的各种不同艺术形式的绘制风格（铅笔、水彩、卡通、国画和油画等）。
                            实验室专注于研究结合传统方法与深度学习方法，探索各种不同艺术形式的非真实感渲染算法。
                            其中，彩铅画是一种特殊形式的非真实感渲染方式，如何准确地表达出彩铅画中独特的色彩特性和纹理效果，
                            从而使生成出的风格兼顾真实感与美观性，是一个具有较大挑战性的问题。
                            <p class="margin-small">&nbsp;</p>
                            相关链接：图像上色、颜色复原相关工作整理汇总（<a href="https://github.com/MarkMoHR/Awesome-Image-Colorization">https://github.com/MarkMoHR/Awesome-Image-Colorization</a>）

                        </p>
                        <br>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/pg2024-song.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="">Controllable Anime Image Editing via Probability of Attribute Tags</a></h3>
                                    <p>
                                        Zhenghao Song, Haoran Mo, and Chengying Gao*
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作提出了一种基于标签概率的可控动画图像编辑框架。在该框架中，我们提出了一个概率编码网络（PEN），用于将概率编码为包含连续变化的特征，这样编码后的特征能够引导预训练扩散模型的生成过程，并可进行线性编辑。同时，我们引入了一个本地编辑模块，其可以自动识别目标区域，并限制仅在这些区域进行编辑，从而保持其他图像区域不变。与现有方法的综合比较表明，我们的框架在一次性和线性编辑模式下有更好的性能。此外，实验进一步证明了本方法的泛化能力。
                                        <br>
                                        <br>
                                        <em>Pacific Graphics (<strong>PG</strong>, 2024) &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="">[论文]</a>
                                        <a href="https://github.com/Songdp-zh/Controllable-Anime-Image-Editing">[代码]</a>
                                    </p>
                                </td>
                            </tr>


                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/cvpr2023.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://arxiv.org/abs/2303.17867">CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer</a></h3>
                                    <p>
                                        Linfeng Wen, Chengying Gao*, Changqing Zou
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作提出一种可逆的风格迁移框架保持内容亲和力。该框架使用基于Channel Refinement模块、无冗余信息的可逆残差网络实现图像-特征空间的双向映射，
                                        结合无偏线性变换模块cWCT对齐风格信息，并通过Matting Laplacian训练损失保持像素亲和力。
                                        本方法通过实验证明该可逆框架能实现内容亲和力的保持、风格化的一致性，在图像、视频处理上优于现有方法。
                                        <br>
                                        <br>
                                        <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>, 2023) &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="https://arxiv.org/abs/2303.17867">[论文]</a>
                                        <a href="https://github.com/linfengWen98/CAP-VSTNet">[代码]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/icme2021inpainting.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://ieeexplore.ieee.org/abstract/document/9428367">Structural Prior Guided Image Inpainting for Complex Scene</a></h3>
                                    <p>
                                        Shuxin Wei, Chengying Gao
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        现有基于深度学习的图像修复方法在面对上下文信息丰富的小面积破损区域已取得较好的修复效果，
                                        然而对复杂场景的大面积破损区域进行修复时往往会产生语义失真、边缘模糊等问题。
                                        本文将复杂场景的图像修复问题分解为语义分割图修复与语义分割图引导的纹理信息修复两阶段，
                                        通过特征相关矩阵评估语义分割图与破损图像已知区域之间的相关性，从而完成破损区域的纹理生成。
                                        <br>
                                        <br>
                                        <em>International Conference on Multimedia & Expo (<strong>ICME</strong>, 2021)  <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://ieeexplore.ieee.org/abstract/document/9428367">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/jisuanjixuebao19.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="">基于稀疏结构的复杂物体修复</a></h3>
                                    <p>
                                        高成英，徐仙儿，罗燕媚，王栋
                                        <br>
                                        <br>
                                        <em>计算机学报</em>，2019
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/Neurocomputing18.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://www.sciencedirect.com/science/article/pii/S0925231218306672">An edge-refined vectorized deep colorization model for grayscale-to-color images</a></h3>
                                    <p>
                                        Zhuo Su, Xiangguo Liang, Jiaming Guo, Chengying Gao, Xiaonan Luo
                                        <br>
                                        <br>
                                        <em>Neurocomputing</em>, 2018
                                        <br>
                                        <a href="https://www.sciencedirect.com/science/article/pii/S0925231218306672">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/pencil-art.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%" class="tdp">
								    <h3> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13334">PencilArt: A Chromatic Penciling Style Generation Framework</a></h3>
                                    <p>
                                        Chengying Gao, Mengyue Tang, Xiangguo Liang, Zhou Su, Changqing Zou
                                        <br>
                                        <br>
                                        <em>Computer Graphics Forum (<strong>CGF</strong>), 2018 &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13334">[论文]</a>
                                    </p>
                                </td>
                            </tr>
                            
<!--                            <tr>-->
<!--                                <td width="30%">-->
<!--								    <img src="./files/images/projects/spic17.png" alt="" width="100%" border="1"/>-->
<!--                                </td>-->
<!--                                <td width="70%">-->
<!--								    <h3> <a href="https://www.sciencedirect.com/science/article/pii/S0923596517300772">Data-Driven Image Completion for Complex Object</a></h3>-->
<!--                                    <p>-->
<!--                                        Chengying Gao, Yanmei Luo, Hefeng Wu*, Dong Wang-->
<!--                                        <br>-->
<!--                                        <br>-->
<!--                                        <em>Signal Processing: Image Communication</em>, 2017-->
<!--                                        <br>-->
<!--                                        <a href="https://www.sciencedirect.com/science/article/pii/S0923596517300772">[论文]</a>-->
<!--                                    </p>-->
<!--                                </td>-->
<!--                            </tr>-->

<!--                            <tr>-->
<!--                                <td width="30%">-->
<!--								    <img src="./files/images/projects/color-transfer.png" alt="" width="100%" border="1"/>-->
<!--                                </td>-->
<!--                                <td width="70%">-->
<!--								    <h3> <a href="https://changqingzou.weebly.com/uploads/5/9/3/6/59369643/wang_et_al-2017-computer_graphics_forum-min.pdf">L0 Gradient-Preserving Color Transfer</a></h3>-->
<!--                                    <p>-->
<!--                                        Dong Wang, Changqing Zou, Guiqing Li, Chengying Gao, Zhuo Su, Ping Tan-->
<!--                                        <br>-->
<!--                                        <br>-->
<!--                                        <em>Computer Graphics Forum (<strong>CGF</strong>), 2017 &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>-->
<!--                                        <br>-->
<!--                                        <a href="https://changqingzou.weebly.com/uploads/5/9/3/6/59369643/wang_et_al-2017-computer_graphics_forum-min.pdf">[论文]</a>-->
<!--                                    </p>-->
<!--                                </td>-->
<!--                            </tr>-->

                        </table>

					</section>

                    <div align="right">
                        <a href="#">Back to top</a>
                    </div>

                    <section id="pose">
						<header class="major">
							<h2 class="head-zh">三维姿势估计与动作生成</h2>
						</header>

                        <p>
                            实验室关注三维姿势、手势估计和三维动作生成等研究方向。
                        </p>
                        <br>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/wang-tvcg-2025.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="">DiFusion: Flexible Stylized Motion Generation Using Digest-and-Fusion Scheme</a></h3>
                                    <p> Yatian Wang, Haoran Mo, Chengying Gao*
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        为了解决现有文本驱动的人体动作合成方法在风格表现方面的不足，我们提出了 DiFusion，一个用于多样化风格动作生成的框架。
                                        该方法能够通过文本灵活控制动作内容，并通过多种模态（文本标签或动作序列）控制风格。
                                        我们的方法采用了双条件动作隐扩散模型，能够在灵活的输入模态下，独立控制内容与风格。
                                        针对文本-动作数据集与风格-动作数据集之间复杂度不平衡的问题，我们提出了 Digest-and-Fusion 训练机制，该机制先从两个数据域中分别汲取领域特定知识，
                                        然后自适应地将它们以兼容的方式融合。全面的评估结果表明，我们的方法在内容对齐度、风格表现力、真实感和多样性等方面均优于现有方法。此外，我们的方法还可扩展至实际应用场景，如动作风格插值等。
                                        <br>
                                        <br>
                                        <em>IEEE Transactions on Visualization and Computer Graphics (<strong>TVCG</strong>, 2025) &nbsp;<strong><font color="#FF0000">(CCF-A)</font></strong></em>
                                        <br>
                                        <a href="">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/icme2022-4.gif" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="./files/ICME2022/Unpaired_Motion_Style_Transfer_camera_ready.pdf">Unpaired Motion Style Transfer with Motion-oriented Projection Flow Network</a></h3>
                                    <p>	Yue Huang, Haoran Mo, Xiao Liang, Chengying Gao*
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        本文提出了一种基于投影流网络和自适应实例归一化的不成对动作风格迁移方法，
                                        利用可逆的投影流网络来投影和还原动作特征，利用无偏的自适应实例归一化来生成风格化特征。
                                        根据动作数据的时序性特点，本文进一步设计了插值模块和引入Transformer的加性耦合层，
                                        以有效提升模型对风格的归纳能力和动作的真实性。
                                        实验表明，该方法可以在保留完整内容的情况下有效迁移动作的风格。
                                        相比现有的的不成对动作风格迁移方法，该模型泛化能力更强，在不可见风格上有更好的聚类效果。
                                        <br>
                                        <br>
                                        <em>International Conference on Multimedia & Expo (<strong>ICME</strong>, 2022) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="./files/ICME2022/Unpaired_Motion_Style_Transfer_camera_ready.pdf">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/neurocomputing2022.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://doi.org/10.1016/j.neucom.2021.12.013">3D interacting hand pose and shape estimation from a single RGB image</a></h3>
                                    <p>	Chengying Gao*, Yujia Yang, Wensheng Li
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作提出了一种基于RGB图像的双手网格重建算法。该算法采用分组卷积来分别提取左右特征，
                                        有效避免了左右手特征互相干扰。此外，本工作还提出了多特征融合模块MF-block，该模块结合了图像特征，
                                        姿势特征和上采样特征，能有效预测被遮挡部分的2D关键点。
                                        最后，本工作提出了基于Transformer机制的3D网格生成模型。
                                        <br>
                                        <br>
                                        <em>Neurocomputing, 2022</em>
                                        <br>
                                        <a href="https://doi.org/10.1016/j.neucom.2021.12.013">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                        </table>
					</section>

                    <div align="right">
                        <a href="#">Back to top</a>
                    </div>

                    <section id="fashion">
                        <header class="major">
							<h2 class="head-zh">服装建模与虚拟试衣</h2>
						</header>

                        <p> 三维服装建模和虚拟试衣在服装制造、影视娱乐和虚拟现实等领域具有广泛的应用，引起了国内外学者的广泛关注。
                            然而由于服装材料、款式及人体体型的多样性和复杂性等问题，目前仍面临着巨大的挑战。
                            近年来实验室专注于服装设计过程中的快速建模和2D、3D的虚拟服装展示算法，
                            并在虚拟试衣和三维服装建模方面取得一些研究成果。

                        </p>
                        <br>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/pg2023.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://onlinelibrary.wiley.com/doi/10.1111/cgf.14938">Controllable Garment Image Synthesis Integrated with Frequency Domain Features</a></h3>
                                    <p>	Xinru Liang, Haoran Mo, Chengying Gao*
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        此工作提出了一个可控的服装图像生成框架。该框架以轮廓草图和纹理块图像为输入，生成具有复杂的、多样的纹理图案的服装图像。
                                        我们使用了基于快速傅里叶变换的生成框架，利用频域特征表示纹理模式的周期信息，以提高全局纹理扩展的性能。
                                        为了进一步提高本框架在细粒度纹理细节上的生成能力，我们还提出了一个频域感知损失，以衡量两个纹理块之间的周期性和规律性的相似性。
                                        <br>
                                        <br>
                                        <em>Computer Graphics Forum (<strong>Pacific Graphics</strong>, 2023) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://onlinelibrary.wiley.com/doi/10.1111/cgf.14938">[论文]</a>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/fashionGAN.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13552">FashionGAN: Display your fashion design using Conditional Generative Adversarial Nets</a></h3>
                                    <p>	Yirui Cui, Qi Liu, Chengying Gao*, Zhuo Su
                                        <br>
                                        <br>
                                        <em>Computer Graphics Forum (<strong>Pacific Graphics</strong>, 2018) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13552">[论文]</a>
                                        <a href="https://github.com/Cuiyirui/FashionGAN">[代码]</a>
										<a href="https://drive.google.com/drive/folders/1DACqCXlJRQxRysO6RVNO7vOoR8YzrjTQ?usp=sharing">[数据集]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/pcm18.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="https://link.springer.com/chapter/10.1007%2F978-3-030-00776-8_25">Automatic 3D Garment Fitting Based on Skeleton Driving</a></h3>
                                    <p> Haozhong Cai, Guangyuan Shi, Chengying Gao*, Dong Wang
                                        <br>
                                        <br>
                                        <em>Pacific-Rim Conference on Multimedia (<strong>PCM</strong>, 2018) <strong><font color="#FF0000">(*oral)</font></strong> &nbsp;<strong><font color="#FF0000">(CCF-C)</font></strong></em>
                                        <br>
                                        <a href="https://link.springer.com/chapter/10.1007%2F978-3-030-00776-8_25">[论文]</a>
                                    </p>
                                </td>
                            </tr>
<!--                            <tr>-->
<!--                                <td width="30%">-->
<!--								    <img src="./files/images/projects/pg14.png" alt="" width="100%" border="1"/>-->
<!--                                </td>-->
<!--                                <td width="70%">-->
<!--								    <h3> <a href="https://diglib.eg.org/handle/10.2312/pgs.20141250.043-048">Automatic Garment Modeling From Front And Back Images</a></h3>-->
<!--                                    <p>	Lifeng Huang, Chengying Gao*-->
<!--                                        <br>-->
<!--                                        <br>-->
<!--                                        <em>Pacific Graphics (<strong>PG</strong>, 2014) &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>-->
<!--                                        <br>-->
<!--                                        <a href="https://diglib.eg.org/handle/10.2312/pgs.20141250.043-048">[论文]</a>-->
<!--                                    </p>-->
<!--                                </td>-->
<!--                            </tr>-->

                        </table>

                    </section>

                    <div align="right">
                        <a href="#">Back to top</a>
                    </div>

					<section id="3d_modeling">
                        <header class="major">
							<h2 class="head-zh">多媒体处理与三维渲染建模</h2>
						</header>
                        <p>
                            在多媒体处理处理方面，实验室关注音乐和舞蹈等内容的生成和理解。
                            <br>
                            在三维渲染与建模方面，实验室关注动态人体建模与神经渲染、基于窄带的快速流体表面重建以及织物建模与渲染等研究方向：
                            <p class="margin-small">&nbsp;</p>
                            <strong>动态人体建模与神经渲染</strong>：基于NeRF的动态人体重建任务通常利用输入的视频序列学习人体的神经表达方式，并对人体进行隐式重建。
                            通过结合体渲染技术，基于输入相机与人体姿势参数生成指定视角与动作的人体渲染图。
                            尽管目前已有大量工作针对该方向展开研究，但是仍存在训练耗时长、渲染质量低、渲染速度慢等问题。
                            <br>
                            <strong>基于窄带的快速流体表面重建</strong>：流体表面网格的重建介于流体模拟和流体渲染之间，
                            重建出来的流体网格质量与最终渲染效果有着极其密切的关系。
							在当前有限的算力前提下，如何在尽可能保证网格质量的同时加快重建效率是一个非常具有挑战性的难题。
                            <br>
                            <strong>织物建模与渲染</strong>：织物是虚拟世界中不可或缺的一部分。
                            基于纤维的纱线几何结构可以极大地还原织物种类的多样性和结构的复杂性，对织物的真实感渲染起着至关重要的作用。
                            然而纤维带来大量的微观细节，这使得庞大的织物模型只能在动画制作、电影特效等离线渲染领域中得以运用，
                            而无法满足电子游戏等实时渲染的需求。近年来实验室专注于找到一种高质量、全自动的三角网格模型到纱线模型的转换算法，
                            进而使用基于微观模型的织物实时渲染算法对纱线模型进行展示，以模拟出织物纤维级别的细节特征。

                        </p>
                        <br>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/zeng_tcsvt.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="">ReGA: Relighting Dynamic Gaussian Avatars from Sparse Views</a></h3>
                                    <p>
                                        Lingzhe Zeng, Wensheng Li, Rongbin Zheng, Chengying Gao*
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        动态人体重照明是一项复杂任务，其核心挑战在于有效处理动态人体几何重建和材质估计。现有工作主要利用神经辐射场实现人体重建和重照明，但这些方法效率较低，而且在材质估计和重照明方面仍然不够准确。本文提出了一种名为ReGA的新方法，该方法利用高效的3D高斯溅射技术，从稀疏视角的人体运动图像序列中构建可动画和可重照明的人体化身。为了克服普通高斯表示法的几何弱点，我们在几何阶段引入了动态对齐机制，结合高斯点云化和基于网格的表示法的优点，生成合理的人体表面。在材质阶段，我们通过引入双重相关性策略来增强逆渲染过程，这些策略建立了高斯辐射颜色和反照率之间的色度相关性。实验表明，我们的方法在动态人体重照明任务中优于现有方法。
                                        <br>
                                        <br>
                                        <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>, 2025) &nbsp;<strong><font color="#FF0000">(中科院1区/CCF-B)</font></strong></em>
                                        <br>
                                        <a href="">[论文]</a>
                                        <br>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/zhu_cgi.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="">Feature Replacement in Gaussian Splatting for 3D Stylization</a></h3>
                                    <p>
                                        Jinkeng Zhu, Wensheng Li, ChengYing Gao*
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        3D生成与编辑作为创意工具，近年来受到了越来越多的关注，其中3D风格化是场景编辑中的一个重要方向。然而，现有的3D风格化方法存在一些限制，包括：需要针对新的风格输入进行重新训练、场景内容与风格信息难以有效分离，以及风格化结果与参考风格图像之间存在不匹配等问题。本文中提出了一种特征替换模块，该模块利用可逆网络对内容特征与风格特征进行解耦，从而实现风格信息的有效替换，同时保留场景内容。此外，本文还引入了一种特征倒角损失，用于对齐生成图像与参考风格图像在高维特征空间中的分布，从而提升风格一致性与视觉协调性。实验结果表明，该方法在生成质量和多视角一致性方面均优于现有技术。
                                        <br>
                                        <br>
                                        <em>Computer Graphics International (<strong>CGI</strong>, 2025) &nbsp;<strong><font color="#FF0000">(CCF-C)</font></strong></em>
                                        <br>
                                        <a href="">[论文]</a>
                                        <br>
                                    </p>
                                </td>
                            </tr>
                            
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/TVCG2024-lws.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="">Efficient Integration of Neural Representations for  Dynamic Humans</a></h3>
                                    <p>
                                        Wensheng Li, Lingzhe Zeng, Chengying Gao, Ning Liu*
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        针对基于NeRF的动态人体重建方法训练效率低的问题，此工作提出学习和整合来自不同空间的动态神经人体表达方式，以实现高效地动态人体重建。具体地，本文提出将存储动态人体特征的高维特征体分解为多个特征平面，随后利用矩阵乘法显式建立这些平面之间的关联性。这种方式使得在优化来自特征平面的插值特征时，能够同步优化各个维度的关联特征，有效地整合了不同维度的观测信息并进一步加速了收敛速度。此外，利用提出的关联优化流程，整合来自不同空间的人体表达方式，并利用观察空间的细节信息逐步细化动态人体的标准表达方式。通过整合多空间人体表达方式，本文进一步促进了多帧的时间相关特征间的协同优化。实验证明了本文提出的方法仅利用大约5分钟的训练时间即可完成动态人体重建，并实现高质量的自由视角渲染。
                                        <br>
                                        <br>
                                        <em>IEEE Transactions on Visualization and Computer Graphics (<strong>TVCG</strong>, 2024) &nbsp;<strong><font color="#FF0000">(中科院 1区/CCF-A)</font></strong></em>
                                        <br>
                                        <a href="">[论文]</a>
                                        <br>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/tmm2024.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="">DanceComposer: Dance-to-Music Generation Using a Progressive Conditional Music Generator</a></h3>
                                    <p>
                                        Xiao Liang, Wensheng Li, Lifeng Huang and Chengying Gao
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        音乐是舞蹈的精髓和灵魂，这推动了从舞蹈自动生成音乐的研究。为了创作与舞蹈匹配的音乐，需考虑舞蹈和音乐的跨模态相关性，如节奏上的协调和风格上的和谐。然而，现有的舞蹈到音乐生成方法很难同时实现节奏对齐和风格匹配。此外，由于缺乏可用的成对数据，生成样本的多样性也很有限。为了解决这些问题，我们提出了一个新的舞蹈到音乐生成框架DanceComposer，它能从舞蹈视频中生成节奏和风格一致的多音轨音乐。DanceComposer采用渐进式条件音乐生成器 (PCMG)，可逐步添加节奏和风格约束，实现节奏对齐和风格匹配。为了加强风格控制，我们引入了共享风格模块（SSM），该模块可学习跨模态特征作为风格约束。这使得PCMG可以在大量纯音乐数据上进行训练，并使生成的作品更加多样化。定量和定性结果表明，我们的方法在整体音乐质量、节奏一致性和风格一致性方面都超越了现有的方法。
                                        <br>
                                        <br>
                                        <em>IEEE Transactions on Multimedia (<strong>TMM</strong>, 2024) &nbsp;<strong><font color="#FF0000">(中科院 1区/CCF-B)</font></strong></em>
                                        <br>
                                        <a href="https://ieeexplore.ieee.org/abstract/document/10539267">[论文]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/icme24-piano.png" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%">
								    <h3> <a href="">PianoBART: Symbolic Piano Music Generation and Understanding with Large-Scale Pre-Training</a></h3>
                                    <p>
                                        Xiao Liang, Zijian Zhao, Weichao Zeng, Yutong He, Fupeng He, Yiyi Wang and Chengying Gao
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        学习音乐结构和作曲模式对于音乐生成和理解都是必要的，但是现有方法不能统一利用所学特征来同时生成和理解音乐。在本文中，我们提出了PianoBART，这是一种基于BART进行符号化钢琴音乐生成和理解的预训练模型。我们针对PianoBART的预训练任务设计了一个多级对象选择策略，可以防止信息泄露或丢失，提高学习能力。预训练中捕获的音乐语义可针对不同音乐生成和音乐理解任务进行微调。实验表明，PianoBART 能高效地学习音乐模式，在生成高质量的连贯乐曲和理解音乐方面表现出色。
                                        <br>
                                        <br>
                                        <em>International Conference on Multimedia & Expo (<strong>ICME</strong>, 2024) &nbsp;<strong><font color="#FF0000">(CCF-B)</font></strong></em>
                                        <br>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/cgi2020.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%" class="tdp">
								    <h3> <a href="https://link.springer.com/article/10.1007/s00371-020-01898-2">A Completely Parallel Surface Reconstruction Method for Particle-Based Fluids</a></h3>
                                    <p>
                                        Wencong Yang, Chengying Gao
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        本文首先提出了一种快速、简单、极其准确的流体表面的窄带方法，使得表面重建算法（例如Marching Cube）
										能够准确地定位到有效的流体表面区域，极大地避免了无用的计算过程。
										同时，我们分析了重建过程潜在的数据竞争和条件分支，利用互斥前缀和算法，
										将整个流体表面重建的过程完全并行化，大大加快了表面重建的效率。
                                        <br>
                                        <br>
                                        <em>Computer Graphics International (<strong>CGI</strong>, 2020) &nbsp;<strong><font color="#FF0000">(CCF-C)</font></strong></em>
                                        <br>
                                        <a href="https://link.springer.com/article/10.1007/s00371-020-01898-2">[论文]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/fabric-model.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%" class="tdp">
								    <h3> <a href="./files/3D-materials/fabric-model.pdf">全自动的纱线模型生成算法</a></h3>
                                    <p>
										张泽坤
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        算法可以自动地将三角网格模型转化为纱线模型。
                                        但算法存在模型适用范围较小、特定类型纱线模型质量较差等缺点，目前正在研究如何解决这些问题。
                                        若问题得以解决，有望改进、简化当前织物工业化生产的设计流程，提高生产效率。
                                        <br>
                                        <br>
                                        <a href="./files/3D-materials/fabric-model.pdf">[详细介绍 (PPT)]</a>
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td width="30%">
								    <img src="./files/images/projects/fabric-render.jpg" alt="" width="100%" border="1"/>
                                </td>
                                <td width="70%" class="tdp">
								    <h3> <a href="./files/3D-materials/fabric-render.pdf">基于微观模型的织物实时渲染算法</a></h3>
                                    <p>
										罗幸荣
                                        <br>
                                        <br>
                                        <strong>简介：</strong>
                                        纤维级织物，是指在纤维级微观尺度下进行建模的织物。
                                        它将织物表示成大量纤维的集合，显式地描述了纤维、单纱以及纱线的相对几何关系。
                                        算法使用核心纤维面片来对单纱中规律缠绕的纤维进行替代，从而减小实时光栅化带来的性能开销，
                                        并与常规纤维相结合，使得能够在可交互的帧率下对纤维级织物进行渲染。
                                        <br>
                                        <br>
                                        <a href="./files/3D-materials/fabric-render.pdf">[详细介绍 (PPT)]</a>
                                    </p>
                                </td>
                            </tr>

                        </table>


                    </section>

                    <div align="right">
                        <a href="#">Back to top</a>
                        <br>
                        <p> &nbsp; </p>
                    </div>

				</div>
			</div>
			
			<!-- Sidebar -->
				<div id="sidebar">
					<div class="inner">

						<!-- Search -->
							<section id="search" class="alt">
								<!--<form method="post" action="#">-->
									<!--<input type="text" name="query" id="query" placeholder="Search" />-->
								<!--</form>-->
                                <div style="text-align:center" >
                                    <img src="./files/images/logo2.png" alt="" width="100%"/>
                                </div>
							</section>

						<!-- Menu -->
							<nav id="menu">
								<header class="major">
									<h2>Menu</h2>
								</header>
								<ul>
									<li><a class="menu-zh" href="index.html">主页</a></li>
									<li><a class="menu-zh" href="projects_indoor.html">研究项目</a></li>
									<li><a class="menu-zh" href="publications_papers.html">论文发表</a></li>
									<li><a class="menu-zh" href="members.html">成员信息</a></li>
									<li><a class="menu-zh" href="students_dev.html">学生发展</a></li>
									<li><a class="menu-zh" href="materials_datasets.html">相关资料</a></li>
									<!--
									<li>
										<span class="opener">Submenu</span>
										<ul>
											<li><a href="#">Lorem Dolor</a></li>
											<li><a href="#">Ipsum Adipiscing</a></li>
											<li><a href="#">Tempus Magna</a></li>
											<li><a href="#">Feugiat Veroeros</a></li>
										</ul>
									</li>
									<li><a href="#">Etiam Dolore</a></li>
									<li><a href="#">Adipiscing</a></li>
									<li>
										<span class="opener">Another Submenu</span>
										<ul>
											<li><a href="#">Lorem Dolor</a></li>
											<li><a href="#">Ipsum Adipiscing</a></li>
											<li><a href="#">Tempus Magna</a></li>
											<li><a href="#">Feugiat Veroeros</a></li>
										</ul>
									</li>
									-->
								</ul>
							</nav>

						<!-- Section -->
						<!--
							<section>
								<header class="major">
									<h2>Ante interdum</h2>
								</header>
								<div class="mini-posts">
									<article>
										<a href="#" class="image"><img src="./images/pic07.jpg" alt="" /></a>
										<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
									</article>
									<article>
										<a href="#" class="image"><img src="./images/pic08.jpg" alt="" /></a>
										<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
									</article>
									<article>
										<a href="#" class="image"><img src="./images/pic09.jpg" alt="" /></a>
										<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
									</article>
								</div>
								<ul class="actions">
									<li><a href="#" class="button">More</a></li>
								</ul>
							</section>
                        -->
						<!-- Section -->
							<section>
								<header class="major">
									<h2 class="head-zh">联系方式</h2>
								</header>
								<ul class="contact">
									<li class="fa-envelope-o">联系邮箱: <a href="#">sysuimsl@126.com</a></li>
									<li class="fa-home">地址: <a href="https://goo.gl/maps/P7iu1XtZfrk5TsKz5">中国 广州, 中山大学, 计算机学院, A409室</a>
									</li>
								</ul>
								<!--<article class="5u 10u$(xsmall) work-item">-->
										<!--<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&d=mhnrYabZI2bz_eHk1W_A8VvNxtAjYBrWfIfxbLnTRPQ&co=4c5459&cmo=faa659'></script>-->
								<!--</article>-->
								<!--<a href="https://info.flagcounter.com/Dsap"><img src="https://s11.flagcounter.com/count2/Dsap/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>-->
							</section>

						<!-- Footer -->
							<footer id="footer">
								<p class="copyright">
									<li>Copyright &copy; 2019-2024 SYSU-IMSL</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</footer>

					</div>
				</div>
        <!-- push the page to Baidu -->
		<script>
		(function(){
		    var bp = document.createElement('script');
		    var curProtocol = window.location.protocol.split(':')[0];
		    if (curProtocol === 'https'){
		   bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
		  }
		  else{
		  bp.src = 'http://push.zhanzhang.baidu.com/push.js';
		  }
		    var s = document.getElementsByTagName("script")[0];
		    s.parentNode.insertBefore(bp, s);
		})();
		</script>
		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
	</body>
</html>
